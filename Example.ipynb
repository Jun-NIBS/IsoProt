{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "source": [
    "# Protocol for analysis of labeled proteomics data\n",
    "We recommend the following structure for describing the protocol and its example workflow. Below you can find an overview of required features of the protocol and its description and rules for best practices.\n",
    "\n",
    "This is the link to the github repository of the protocol \n",
    "TODO add version!\n",
    "https://github.com/ProtProtocols/biocontainer-jupyter\n",
    "\n",
    "Link to docker image:\n",
    "\n",
    "\n",
    "\n",
    "## Abstract\n",
    "\n",
    "Labelled peptide mass spectrometry provides fast large-scale comparison of protein abundances over multiple conditions. To date, no one stop shop software solution exists that enables the common researcher to carry out the full analysis of the acquired raw data. Pipeline for this analysis have often been established in laboratories that are based on a combination of different software tools and in-house programs. Different and often new versions of the used tools and issues with the compatibility of apparently interoperable tools make it very difficult to ensure reproducible proteomics data analysis. We present a 100% reproducible software protocol to fully analyze data from one of the most popular types of proteomics experiments. The protocol is fully based on open source tools installed on a docker container, additionally providing a user-friendly and interactive browser interface for guidance of configuration and execution of the different operations. An example use case is provided that can be used for testing and adaption of own data sets. With this setup, analysis of labelled MS data will yield identical results on any computer that meets the computational bandwidth to run the analysis. \n",
    "\n",
    "(Provide a short description of the software protocol including broader context, functionality, use case and purpose.)\n",
    "\n",
    "\n",
    "## Maintainer\n",
    "Provide details about the protocol maintainer (e.g. email address and/or github username)\n",
    "\n",
    "## Software\n",
    "Specify links for documentation and tutorials of used software, source code, publications and use cases. Detail versions of each used software. Alternatively, provide links to the software descriptions in https://bio.tools where this information is available.\n",
    "\n",
    "## Diagram\n",
    "Provide a simple diagram of functionality of the workflow/software. We recommend using controlled vocabularies for input/output data types and file formats as well as provided operation of the tool(s). You can use http://edamontology.org terms for the description.\n",
    "\n",
    "__TODO: example__\n",
    "\n",
    "## System requirements\n",
    "Fill in the following items:\n",
    "Required hard disk space for docker image, input and output files: \n",
    "\n",
    "Required memory: \n",
    "\n",
    "Recommmended number of threads: \n",
    "\n",
    "## Example \n",
    "Presentation of well-documented instructions and commands to run the example use case. Depending on the use case and the software, provide link(s) to open the web service incorportated in the Docker image (e.g. 0.0.0.0:8080), bash commands to run programs from the command line and additional code for e.g. checking and visualizing the (intermediate) results. \n",
    "\n",
    "The example data file is an extract of spectra from the iTRAQ 8-plex data in ref. https://doi.org/10.1371/journal.pone.0137048\n",
    "\n",
    "\n",
    "Instead of providing the instructions in this notebook, one can also provide a link to a notebook containing the example use case.\n",
    "\n",
    "## More general use case (optional)\n",
    "Provide link to notebook with a generalized use case that easily can be adapted to e.g. process different input data and concurrent parametrization.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "source": [
    "# Example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-16T17:24:03.705969Z",
     "start_time": "2018-05-16T17:24:03.701613Z"
    },
    "hideCode": true,
    "hideOutput": true,
    "hidePrompt": true,
    "hide_input": true,
    "init_cell": true,
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "%load_ext rpy2.ipython"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "source": [
    "Specify parameters for database search and evaluation of identified peptide-spectrum matches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-16T17:24:10.610933Z",
     "start_time": "2018-05-16T17:24:03.709024Z"
    },
    "code_folding": [
     153,
     255,
     279,
     311,
     543,
     562,
     568
    ],
    "hideCode": true,
    "hidePrompt": true,
    "hide_input": true,
    "init_cell": true,
    "run_control": {
     "marked": true
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from ipywidgets import VBox, Label\n",
    "import sys, os\n",
    "import subprocess\n",
    "import psutil\n",
    "import pandas as pd\n",
    "from IPython.display import Javascript, display\n",
    "import json\n",
    "\n",
    "# create an empty class as a storage for all UI widgets\n",
    "class SearchUI:\n",
    "    def __init__(self):\n",
    "        # working directory\n",
    "        self.work_dir_select = widgets.Dropdown(options={'/data/': '/data/', 'Example files': '/home/biodocker/'}, \n",
    "                                                value='/home/biodocker/')\n",
    "        self.work_dir_select.observe(observe_work_dir_select)\n",
    "        \n",
    "        # basic search settings\n",
    "        self.precursor_tolerance = widgets.IntSlider(min=-10,max=30,step=1,value=10)\n",
    "        self.fragment_tolerance = widgets.BoundedFloatText(min=0,max=200,value=0.05)\n",
    "        self.fasta_db = widgets.Dropdown(options={\"sp_human.fasta\": \"IN/sp_human.fasta\"})\n",
    "        \n",
    "        self.generate_decoy = widgets.Checkbox(value=True, description=\"Generate decoy sequences\")\n",
    "        self.target_fdr = widgets.BoundedFloatText(min=0,max=1,value=0.01)\n",
    "        \n",
    "        # TODO  needs table to describe labeling formats\n",
    "        self.labelling = widgets.Dropdown(options=\n",
    "                          {'TMT6': 'TMT 6-plex of K,TMT 6-plex of peptide N-term',\n",
    "                           'TMT10': 'TMT 10-plex of K,TMT 10-plex of peptide N-term',\n",
    "                           'iTRAQ4 (Y fixed)': 'iTRAQ 4-plex of K,iTRAQ 4-plex of Y,iTRAQ 4-plex of peptide N-term',\n",
    "                           'iTRAQ4 (Y variable)': 'iTRAQ 4-plex of K,iTRAQ 4-plex of peptide N-term',\n",
    "                           'iTRAQ8 (Y fixed)': 'iTRAQ 8-plex of K,iTRAQ 8-plex of Y,iTRAQ 8-plex of peptide N-term',\n",
    "                           'iTRAQ8 (Y variable)': 'iTRAQ 8-plex of K,iTRAQ 8-plex of peptide N-term'},\n",
    "                      value='iTRAQ 8-plex of K,iTRAQ 8-plex of peptide N-term')\n",
    "        \n",
    "        self.missed_cleavages = widgets.IntSlider(min=0,max=10,step=1,value=1)\n",
    "        self.fixed_ptms = widgets.Dropdown(options=[\"Carbamidomethylation of C\",\"None\"])\n",
    "\n",
    "        # PTMs\n",
    "        self.var_ptms = widgets.SelectMultiple(\n",
    "            options=[\"Oxidation of M\",\n",
    "                     \"Phosphorylation of STY\",\n",
    "                     \"Acetylation of peptide N-term\",\n",
    "                     \"Acetylation of protein N-term\"],\n",
    "            value=['Oxidation of M'])\n",
    "        \n",
    "        self.spectra_dir = widgets.Dropdown(options={\"IN\": \"IN\"})\n",
    "        \n",
    "        # basic stat input fields\n",
    "        self.summarization_method = widgets.Dropdown(options=\n",
    "                                                    {\"Median of all PSMs\": \"median\",\n",
    "                                                     \"Average of all PSMs\": \"average\",\n",
    "                                                     \"Top 3 PSMs (Median)\": \"top3\",\n",
    "                                                     \"iBAQ to combine peptides\": \"ibaq\"},\n",
    "                                                    value='ibaq')\n",
    "        self.min_protein_psms = widgets.IntSlider(min=0,max=10,step=1,value=1)\n",
    "        self.use_ptms_for_quant = widgets.Checkbox(value=True, description=\"Use PTMs for quantification\")\n",
    "\n",
    "        # button to show experimental design\n",
    "        self.exp_des_button = widgets.Button(\n",
    "            description='Enter design',\n",
    "            disabled=False,\n",
    "            button_style='', # 'success', 'info', 'warning', 'danger' or ''\n",
    "            tooltip='Enter experimental design',\n",
    "            icon='check'\n",
    "        )\n",
    "\n",
    "        self.exp_des_button.on_click(show_exp_design)\n",
    "        \n",
    "        \n",
    "    def updateFastaFiles(self, workdir):\n",
    "        # get all FASTA files\n",
    "        fasta_files = [file for file in os.listdir(workdir) if file[-6:] == \".fasta\"]\n",
    "        \n",
    "        # also search all subdirectories for FASTA files\n",
    "        for d in os.listdir(workdir):\n",
    "            d_path = os.path.join(workdir, d)\n",
    "            if os.path.isdir(d_path) and d[0] != \".\":\n",
    "                fasta_files += [os.path.join(d, file) for file in os.listdir(d_path) if file[-6:] == \".fasta\"]\n",
    "        \n",
    "        # create the dict to add as values to the control\n",
    "        file_list = dict()\n",
    "        sel_value = None\n",
    "        \n",
    "        for f in fasta_files:\n",
    "            file_list[f] = os.path.join(os.path.abspath(workdir), f)\n",
    "            if sel_value is None:\n",
    "                sel_value = os.path.join(os.path.abspath(workdir), f)\n",
    "        \n",
    "        self.fasta_db.options = file_list\n",
    "        self.fasta_db.value = sel_value\n",
    "        \n",
    "        # update the list of possible peaklist directories\n",
    "        directories = [d for d in os.listdir(workdir) if os.path.isdir(os.path.join(workdir, d)) and d[0] != \".\"]\n",
    "        \n",
    "        dir_list = dict()\n",
    "        \n",
    "        for d in directories:\n",
    "            dir_list[d] = os.path.join(os.path.abspath(workdir), d)\n",
    "            \n",
    "        self.spectra_dir.options = dir_list\n",
    "        \n",
    "        if \"IN\" in dir_list:\n",
    "            self.spectra_dir.value = dir_list[\"IN\"]\n",
    "        \n",
    "        \n",
    "    def display(self):\n",
    "        self.updateFastaFiles(self.work_dir_select.value)\n",
    "        \n",
    "        settings_box = VBox([Label('Precursor tolerance (ppm):'), self.precursor_tolerance, \n",
    "                             Label('Fragment ion tolerance (da):'), self.fragment_tolerance,\n",
    "                             Label('Number of miscleavages;'), self.missed_cleavages,\n",
    "                             Label('Further fixed modifications'), self.fixed_ptms,\n",
    "                             Label('Further variable modifications (Hold Ctrl to select multiple)'), self.var_ptms,\n",
    "                             Label('Fasta file (database, must NOT contain decoy sequences):'), self.fasta_db,\n",
    "                             self.generate_decoy,\n",
    "                             Label('Target (protein, peptide, and PSM) FDR:'), self.target_fdr,\n",
    "                             Label('Quantification method:'), self.labelling,\n",
    "                             Label('Summarization method:'), self.summarization_method,\n",
    "                             Label('Minimum number of PSMs per protein:'), self.min_protein_psms,\n",
    "                             self.use_ptms_for_quant,\n",
    "                             Label('Working directory'), self.work_dir_select,\n",
    "                             Label('Folder for spectra files (files need to be mgf)'), self.spectra_dir,\n",
    "                             Label('Note: When entering the experimental design, the working directory and '\n",
    "                                   'labelling method can no longer be changed.'),\n",
    "                             self.exp_des_button])\n",
    "\n",
    "        display(settings_box)\n",
    "        \n",
    "    def save_config(self, file):\n",
    "        \"\"\"\n",
    "        Save all configurations set by the user into a JSON formatted\n",
    "        text file.\n",
    "        :param: file: The name of the target file to use. Will be overwritten if it exists.\n",
    "        \"\"\"\n",
    "        search_config = dict()\n",
    "        search_config[\"work_dir\"] = self.work_dir_select.value\n",
    "        search_config[\"precursor_tolerance\"] = self.precursor_tolerance.value\n",
    "        search_config[\"fragment_tolerance\"] = self.fragment_tolerance.value\n",
    "        search_config[\"fasta_file\"] = self.fasta_db.value\n",
    "        search_config[\"generate_decoy\"] = self.generate_decoy.value\n",
    "        search_config[\"quantification_method\"] = self.labelling.value\n",
    "        search_config[\"missed_cleavages\"] = self.missed_cleavages.value\n",
    "        search_config[\"fixed_mods\"] = self.fixed_ptms.value\n",
    "        search_config[\"var_mods\"] = self.var_ptms.value\n",
    "        search_config[\"summarization_method\"] = self.summarization_method.value\n",
    "        search_config[\"min_protein_psms\"] = self.min_protein_psms.value\n",
    "        search_config[\"use_ptms_for_quant\"] = self.use_ptms_for_quant.value\n",
    "        search_config[\"target_fdr\"] = self.target_fdr.value\n",
    "        \n",
    "        json_string = json.dumps(search_config)\n",
    "        \n",
    "        with open(file, \"w\") as writer:\n",
    "            writer.write(json_string + \"\\n\")\n",
    "        \n",
    "\n",
    "class ExpDesignUI:\n",
    "    def __init__(self, labelling_technique, result_file):\n",
    "        \"\"\"\n",
    "        Generates all use interface objects as member variables.\n",
    "        \n",
    "        :param labelling_technique: The labelling method used.\n",
    "        \"\"\"\n",
    "        self.result_file = result_file\n",
    "        \n",
    "        # always expect two groups\n",
    "        self.group1_name = widgets.Text(placeholder = \"Treatment\", description = \"Group 1:\")\n",
    "        self.group2_name = widgets.Text(placeholder = \"Control\", description = \"Group 2:\")\n",
    "        \n",
    "        self.channels = {\n",
    "            'TMT6': [\"126\", \"127\", \"128\", \"129\", \"130\", \"131\"],\n",
    "            'TMT10': [\"126\", \"127N\", \"127C\", \"128N\", \"128C\", \"129N\", \"129C\", \"130N\", \"130C\", \"131\"],\n",
    "            'iTRAQ4': [\"114\", \"115\", \"116\", \"117\"],\n",
    "            'iTRAQ8': [\"113\", \"114\", \"115\", \"116\", \"117\", \"118\", \"119\", \"121\"]\n",
    "        }\n",
    "        \n",
    "        # removed everything in string labellign_technique after space \n",
    "        if labelling_technique.split(\" \")[0] not in self.channels:\n",
    "            raise Exception(\"Unknown labelling technique: '\" + labelling_technique + \"'\")\n",
    "            \n",
    "        self.labelling_technique = labelling_technique.split(\" \")[0]\n",
    "            \n",
    "        # generate the textfields for the channels\n",
    "        self.channel_names = list()\n",
    "        \n",
    "        for channel in self.channels[self.labelling_technique]:\n",
    "            self.channel_names.append(widgets.Text(description = channel, placeholder = \"Sample \" + channel))\n",
    "            \n",
    "        # add select boxes to select the experimental group\n",
    "        self.group_selects = list()\n",
    "        \n",
    "        for channel in self.channels[self.labelling_technique]:\n",
    "            self.group_selects.append(widgets.Dropdown(options = [\"Group 1\", \"Group 2\"], value = \"Group 1\"))\n",
    "            \n",
    "        self.save_button = widgets.Button(\n",
    "            description='Save design',\n",
    "            disabled=False,\n",
    "            button_style='', # 'success', 'info', 'warning', 'danger' or ''\n",
    "            tooltip='Save the experimental design',\n",
    "            icon='check'\n",
    "        )\n",
    "\n",
    "        self.save_button.on_click(self.save_design)\n",
    "        \n",
    "        self.search_button_visible = False\n",
    "\n",
    "    def display(self):\n",
    "        widget_list = [widgets.Label(\"Treatment group names:\"), self.group1_name, self.group2_name,\n",
    "                       widgets.Label(\"Sample names (per channel):\")]\n",
    "        \n",
    "        for i in range(0, len(self.channel_names)):\n",
    "            widget_list.append(widgets.HBox([self.channel_names[i], self.group_selects[i]]))\n",
    "            \n",
    "        widget_list.append(self.save_button)\n",
    "        \n",
    "        widget_box = VBox(widget_list)\n",
    "        \n",
    "        display(widget_box)\n",
    "        \n",
    "    def save_design(self, button):\n",
    "        # get all names\n",
    "        sample_names = [s.value if s.value != \"\" else s.placeholder for s in self.channel_names]\n",
    "        sample_group = [g.value for g in self.group_selects]\n",
    "        channel_names = self.channels[self.labelling_technique]\n",
    "        \n",
    "        # TODO: make sure all sample names are filled in\n",
    "        \n",
    "        # replace the group names\n",
    "        for i in range(0, len(sample_group)):\n",
    "            if sample_group[i] == \"Group 1\":\n",
    "                sample_group[i] = self.group1_name.value if self.group1_name.value != \"\" else self.group1_name.placeholder\n",
    "            elif sample_group[i] == \"Group 2\":\n",
    "                sample_group[i] = self.group2_name.value if self.group2_name.value != \"\" else self.group2_name.placeholder\n",
    "        \n",
    "        design_data = pd.DataFrame(\n",
    "            data={'channel': channel_names, 'sample_name' : sample_names, 'sample_group': sample_group},\n",
    "        )\n",
    "        \n",
    "        design_data.to_csv(path_or_buf=self.result_file, sep=\"\\t\", index=False)\n",
    "        \n",
    "        # add the run search button\n",
    "        if not self.search_button_visible:\n",
    "            self.search_button = widgets.Button(\n",
    "                description='Run search',\n",
    "                disabled=False,\n",
    "                button_style='', # 'success', 'info', 'warning', 'danger' or ''\n",
    "                tooltip='Run the search',\n",
    "                icon='check'\n",
    "            )\n",
    "            \n",
    "            self.search_button.on_click(run_search)\n",
    "\n",
    "            search_box = VBox([self.search_button])\n",
    "            display(search_box)\n",
    "            \n",
    "            self.search_button_visible = True\n",
    "      \n",
    "        \n",
    "def adapt_mgf_titles(filenames):\n",
    "    \"\"\"\n",
    "    This function changes all MGF titles to [filename].[spec index]\n",
    "    :param: filenames: Filenames of the MGF files to change\n",
    "    \"\"\"\n",
    "    for filename in filenames:\n",
    "        with open(filename, \"r\") as reader:\n",
    "            clean_name = os.path.basename(filename).replace(\" \", \"_\")\n",
    "            # MGF index reference in PSI standard is 1-based\n",
    "            cur_index = 1\n",
    "            \n",
    "            with open(filename + \".tmp\", \"w\") as writer:\n",
    "                for line in reader:\n",
    "                    if line[0:6] == \"TITLE=\":\n",
    "                        writer.write(\"TITLE=\" + clean_name + \".\" + str(cur_index) + \"\\n\")\n",
    "                        cur_index += 1\n",
    "                    else:\n",
    "                        writer.write(line)\n",
    "                        \n",
    "        # backup the original file\n",
    "        os.rename(filename, filename + \".org\")\n",
    "        os.rename(filename + \".tmp\", filename)\n",
    "        \n",
    "        \n",
    "def filter_mgf_peaks(filenames, min_mz=100, max_mz=150):\n",
    "    \"\"\"\n",
    "    Removes all peaks from the passed mgf files that are below min_mz or\n",
    "    above max_mz. The results are written to files with the same name but\n",
    "    \".filtered\" appended to the name.\n",
    "    :param: filenames: List of filenames to process.\n",
    "    :param: min_mz: Minimum m/z a peak must have to be kept\n",
    "    :param: max_mz: Maximum m/z a peak may have to be kept\n",
    "    \"\"\"\n",
    "    for filename in filenames:\n",
    "        with open(filename, \"r\") as reader:\n",
    "            with open(filename + \".filtered\", \"w\") as writer:\n",
    "                for line in reader:\n",
    "                    # check if it's a peak\n",
    "                    if line[0].isdigit():\n",
    "                        sep_index = line.find(\" \")\n",
    "                        if sep_index < 0:\n",
    "                            sep_index = line.find(\"\\t\")\n",
    "                        if sep_index < 0:\n",
    "                            raise Exception(\"Invalid peak definition found: \" + line + \n",
    "                                            \". Failed to filter file \" + filename)\n",
    "                            \n",
    "                        mz = float(line[:sep_index])\n",
    "                            \n",
    "                        # ignore any non-matching peaks\n",
    "                        if mz < min_mz or mz > max_mz:\n",
    "                            continue\n",
    "                            \n",
    "                    # copy the line\n",
    "                    writer.write(line)\n",
    "    \n",
    "\n",
    "def show_exp_design(button):\n",
    "    \"\"\"\n",
    "    Display the experimental design dialog. This function is called\n",
    "    through the button at the end of the SearchUI dialog\n",
    "    \"\"\"\n",
    "    global searchUI, expDesignUI\n",
    "    \n",
    "    work_dir = os.path.abspath(os.path.join(searchUI.work_dir_select.value, \"OUT\"))\n",
    "    \n",
    "    if not os.path.isdir(work_dir):\n",
    "        os.mkdir(work_dir)\n",
    "    \n",
    "    # disable the exp_design button and the labelling method\n",
    "    searchUI.exp_des_button.disabled = True\n",
    "    searchUI.labelling.disabled = True\n",
    "    searchUI.work_dir_select.disabled = True\n",
    "    \n",
    "    # get the currently selected labelling method\n",
    "    labelling_method = list(searchUI.labelling.options.keys())[searchUI.labelling.index]\n",
    "\n",
    "    expDesignUI = ExpDesignUI(labelling_method, result_file=os.path.join(work_dir, \"exp_design.tsv\"))\n",
    "    expDesignUI.display() \n",
    "    \n",
    "    \n",
    "def run_search(button):\n",
    "    global searchUI, result_file\n",
    "    \n",
    "    # get the free memory in MB\n",
    "    free_mem = round(psutil.virtual_memory().available / 1024 / 1024)\n",
    "    # if there's more than 1G available, leave 1G for other tasks\n",
    "    if free_mem > 1000:\n",
    "        free_mem -= 1000\n",
    "    \n",
    "    # make sure all required fields were selected\n",
    "    if searchUI.work_dir_select.value is None:\n",
    "        print(\"Error: No working directory selected\")\n",
    "        return\n",
    "    \n",
    "    if searchUI.fasta_db.value is None:\n",
    "        print(\"Error: No FASTA file selected\")\n",
    "        return\n",
    "    \n",
    "    # create the directory paths to work in\n",
    "    peaklist_dir = os.path.abspath(searchUI.spectra_dir.value)\n",
    "    \n",
    "    if not os.path.isdir(peaklist_dir):\n",
    "        raise Exception(\"Invalid peak list directory selected: \" + peaklist_dir + \" does not exist.\")\n",
    "    \n",
    "    peptide_shaker_jar = \"/home/biodocker/bin/PeptideShaker-1.16.17/PeptideShaker-1.16.17.jar\"\n",
    "    searchgui_jar = \"/home/biodocker/bin/SearchGUI-3.2.20/SearchGUI-3.2.20.jar\"\n",
    "    work_dir = os.path.abspath(os.path.join(searchUI.work_dir_select.value, \"OUT\"))\n",
    "    exp_design_file = os.path.join(work_dir, \"exp_design.tsv\")\n",
    "    \n",
    "    # the searches should be performed in the \"OUT\" directory\n",
    "    if not os.path.isdir(work_dir):\n",
    "        os.mkdir(work_dir)\n",
    "    else:\n",
    "        # make sure all temporary files are gone\n",
    "        tmp_files =  [\"AllQuantPSMs.RData\", \"AllQuantPSMs.csv\", \"experiment.cpsx\", \"experiment1_test_1_Extended_PSM_Report.txt\",\n",
    "                     \"search.par\", \"searchgui_out.zip\"]\n",
    "        for tmp_file in tmp_files:\n",
    "            complete_name = os.path.join(work_dir, tmp_file)\n",
    "            if os.path.isfile(complete_name):\n",
    "                os.remove(complete_name)\n",
    "    \n",
    "    # -------------------------------------\n",
    "    # Save the settings\n",
    "    searchUI.save_config(os.path.join(work_dir, \"search_settings.json\"))\n",
    "    \n",
    "    # -------------------------------------\n",
    "    # Fix all MGF titles\n",
    "    print(\"Adapting MGF titles...\")\n",
    "    mgf_filenames = [os.path.join(peaklist_dir, f) for f in os.listdir(peaklist_dir) if f[-4:].lower() == \".mgf\"]\n",
    "    adapt_mgf_titles(mgf_filenames)\n",
    "    print(mgf_filenames)\n",
    "    \n",
    "    print(\"Extracting reporter peaks...\")\n",
    "    filter_mgf_peaks(mgf_filenames)\n",
    "    \n",
    "        \n",
    "    # -------------------------------------\n",
    "    # Generate the decoy database\n",
    "    \n",
    "    if searchUI.generate_decoy.value == True:\n",
    "        print(\"Creating decoy database...\")\n",
    "\n",
    "        # create the decoy database\n",
    "        subprocess.run([\"java\", \"-Xmx\" + str(free_mem) + \"M\", \"-cp\", searchgui_jar, \n",
    "                       \"eu.isas.searchgui.cmd.FastaCLI\", \"-in\", searchUI.fasta_db.value, \"-decoy\"], check=True,\n",
    "                       cwd=work_dir,\n",
    "                       stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "\n",
    "        # get the filename of the decoy database\n",
    "        database_file = os.path.abspath(searchUI.fasta_db.value)[:-6] + \"_concatenated_target_decoy.fasta\"\n",
    "    else:\n",
    "        # simply use the selected database file\n",
    "        database_file = os.path.abspath(searchUI.fasta_db.value)\n",
    "    \n",
    "    if not os.path.isfile(database_file):\n",
    "        raise Exception(\"Failed to find generated decoy database\")\n",
    "        \n",
    "    # ---------------------------------------------\n",
    "    # Create the search parameter file\n",
    "        \n",
    "    # build the arguments to create the parameter file\n",
    "    param_file = os.path.join(work_dir, \"search.par\")\n",
    "    \n",
    "    # remove any old parameters\n",
    "    if os.path.isfile(param_file):\n",
    "        os.remove(param_file)\n",
    "    \n",
    "    search_args = [\"java\", \"-Xmx\" + str(free_mem) + \"M\", \"-cp\", searchgui_jar,\n",
    "                   \"eu.isas.searchgui.cmd.IdentificationParametersCLI\",\n",
    "                   \"-out\", param_file]\n",
    "    \n",
    "    # precursor tolerance\n",
    "    search_args.append(\"-prec_tol\")\n",
    "    search_args.append(str(searchUI.precursor_tolerance.value))\n",
    "    # fragment tolerance\n",
    "    search_args.append(\"-frag_tol\")\n",
    "    search_args.append(str(searchUI.fragment_tolerance.value))\n",
    "    # fixed mods\n",
    "    # TODO: labelling cannot always be set as fixed mod???\n",
    "    fixed_mod_string = str(searchUI.labelling.value) + \",\" + str(searchUI.fixed_ptms.value)\n",
    "    search_args.append(\"-fixed_mods\")\n",
    "    search_args.append(fixed_mod_string)\n",
    "    # database\n",
    "    search_args.append(\"-db\")\n",
    "    search_args.append(database_file)\n",
    "    # missed cleavages\n",
    "    search_args.append(\"-mc\")\n",
    "    search_args.append(str(searchUI.missed_cleavages.value))\n",
    "    \n",
    "    # var mods\n",
    "    labelling_method = list(searchUI.labelling.options.keys())[searchUI.labelling.index]\n",
    "    \n",
    "    if len(searchUI.var_ptms.value) > 0 or (\"Y variable\" in labelling_method) :\n",
    "        search_args.append(\"-variable_mods\")\n",
    "        var_mod_list = list()\n",
    "        \n",
    "        for var_mod in searchUI.var_ptms.value:\n",
    "            if var_mod == \"Phosphorylation of STY\":\n",
    "                var_mod_list += [\"Phosphorylation of S\", \"Phosphorylation of T\", \"Phosphorylation of Y\"]\n",
    "            else:\n",
    "                var_mod_list.append(var_mod)\n",
    "        if (labelling_method == \"iTRAQ4 (Y variable)\"):\n",
    "            var_mod_list.append(\"iTRAQ 4-plex of Y\")\n",
    "        if (labelling_method == \"iTRAQ8 (Y variable)\"):\n",
    "            var_mod_list.append(\"iTRAQ 8-plex of Y\")\n",
    "        \n",
    "                \n",
    "        search_args.append(\",\".join(var_mod_list))\n",
    "        \n",
    "    # create the search parameter file\n",
    "    print(\"Creating search parameter file...\")\n",
    "    # print(\" \".join(search_args))\n",
    "    subprocess.run(search_args, check=True, cwd=work_dir, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    \n",
    "    if not os.path.isfile(param_file):\n",
    "        raise Exception(\"Failed to create search parameters\")\n",
    "        \n",
    "    # ------------------------------------------------\n",
    "    # Run the search\n",
    "    print(\"Running search...\")\n",
    "    # TODO: create list of spectrum files - or the folder\n",
    "    spectrum_files = peaklist_dir\n",
    "    print(\"  Searching files in \" + spectrum_files)\n",
    "    search_process = subprocess.run([\"java\", \"-Xmx\" + str(free_mem) + \"M\", \"-cp\", searchgui_jar,\n",
    "                    \"eu.isas.searchgui.cmd.SearchCLI\", \"-spectrum_files\", spectrum_files,\n",
    "                    \"-output_folder\", work_dir, \"-id_params\", param_file,\n",
    "                    \"-xtandem\", \"0\", \"-msgf\", \"1\", \"-comet\", \"0\", \"-ms_amanda\", \"0\", \n",
    "                    \"-myrimatch\", \"0\", \"-andromeda\", \"0\", \"-omssa\", \"0\", \"-tide\", \"0\"],\n",
    "                    check=False, cwd=work_dir, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, universal_newlines=True)\n",
    "    \n",
    "    if search_process.returncode != 0:\n",
    "        print(search_process.stdout)\n",
    "        raise Exception(\"Search process failed.\")\n",
    "    \n",
    "    print(\"Search completed.\")\n",
    "    \n",
    "    # -------------------------------------------------\n",
    "    # Run PeptideShaker\n",
    "    print(\"Processing result using PeptideShaker...\")\n",
    "    peptide_shaker_result_file = os.path.join(work_dir, \"experiment.cpsx\")\n",
    "\n",
    "    peptide_shaker_process = subprocess.run([\"java\", \"-Xmx\" + str(free_mem) + \"M\", \"-cp\", peptide_shaker_jar,\n",
    "                    \"eu.isas.peptideshaker.cmd.PeptideShakerCLI\",\n",
    "                    \"-useGeneMapping\", \"0\",\n",
    "                    \"-experiment\", \"experiment1\",\n",
    "                    \"-sample\", \"test\",\n",
    "                    \"-replicate\", \"1\",\n",
    "                    \"-identification_files\", work_dir,\n",
    "                    \"-out\", peptide_shaker_result_file,\n",
    "                    \"-id_params\", param_file,\n",
    "                    \"-spectrum_files\", spectrum_files],\n",
    "                    check=False, cwd=work_dir, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, universal_newlines=True)\n",
    "    \n",
    "    if peptide_shaker_process.returncode != 0:\n",
    "        print(peptide_shaker_process.stdout)\n",
    "        raise Exception(\"Failed to run PeptideShaker\")\n",
    "\n",
    "    if not os.path.isfile(peptide_shaker_result_file):\n",
    "        raise Exception(\"Failed to process result file.\")\n",
    "      \n",
    "    # ---------------------------------------------------\n",
    "    # create TSV output files\n",
    "    print(\"Converting result to TSV format...\")\n",
    "    conversion_process = subprocess.run([\"java\", \"-Xmx\" + str(free_mem) + \"M\", \"-cp\", peptide_shaker_jar,\n",
    "                  \"eu.isas.peptideshaker.cmd.ReportCLI\",\n",
    "                  \"-in\", peptide_shaker_result_file,\n",
    "                  \"-out_reports\", work_dir,\n",
    "                  \"-reports\", \"8\"],\n",
    "                  check=False, cwd=work_dir, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, universal_newlines=True)\n",
    "    \n",
    "    if conversion_process.returncode != 0:\n",
    "        print(conversion_process.stdout)\n",
    "        raise Exception(\"Conversion process failed\")\n",
    "    \n",
    "    result_file=os.path.join(work_dir, \"experiment1_test_1_Extended_PSM_Report.txt\")\n",
    "    \n",
    "    if not os.path.isfile(result_file):\n",
    "        raise Exception(\"Error: Conversion failed\")\n",
    "        \n",
    "    # create parameter list as input for R script\n",
    "    global Rinput\n",
    "    Rinput = [searchUI.labelling.value, searchUI.spectra_dir.value, work_dir, \n",
    "              searchUI.summarization_method.value, searchUI.min_protein_psms.value,\n",
    "              searchUI.use_ptms_for_quant.value, searchUI.target_fdr.value]\n",
    "    \n",
    "    print(\"Done.\") \n",
    "    \n",
    "    on_search_completed()\n",
    "\n",
    "    \n",
    "def on_search_completed():\n",
    "    \"\"\"\n",
    "    This function is called once the search is successfully complete.\n",
    "    \"\"\"\n",
    "    global Rinput, expDesignUI\n",
    "    \n",
    "    # disable previous steps\n",
    "    expDesignUI.search_button.disabled = True\n",
    "    expDesignUI.save_button.disabled = True\n",
    "    \n",
    "    # add the new button to run the R scripts\n",
    "    run_quant_button = widgets.Button(\n",
    "        description=\"Run quantification and peptide inference\",\n",
    "        layout=widgets.Layout(width='30%'))\n",
    "    \n",
    "    run_quant_button.on_click(run_next_cells)\n",
    "    display(run_quant_button)\n",
    "\n",
    "    \n",
    "def run_next_cells(button):\n",
    "    # Run the javascript code to launch the next cell\n",
    "    display(\n",
    "        Javascript('IPython.notebook.execute_cell_range(IPython.notebook.get_selected_index()+1, IPython.notebook.get_selected_index()+6)'))\n",
    "    \n",
    "    \n",
    "def observe_work_dir_select(change):\n",
    "    global work_dir_select\n",
    "    if change['type'] == \"change\" and change['name'] == \"value\":\n",
    "        # update the required UI controls\n",
    "        searchUI.updateFastaFiles(change[\"new\"])\n",
    "\n",
    "# -------------------\n",
    "# Code to create the UI\n",
    "# --------------------\n",
    "result_file=None\n",
    "work_dir=None\n",
    "peaklist_dir=os.path.abspath(\"IN\")\n",
    "\n",
    "searchUI = SearchUI()\n",
    "searchUI.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-16T17:26:05.233704Z",
     "start_time": "2018-05-16T17:25:48.524228Z"
    },
    "code_folding": [],
    "hideCode": true,
    "hidePrompt": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%R -i Rinput --width=1000\n",
    "\n",
    "## assuming that the computer allows min. 4 threads\n",
    "NumThreads <- 4\n",
    "\n",
    "print(Rinput)\n",
    "#stop()\n",
    "\n",
    "# reading parameters\n",
    "labeling <- Rinput[[1]]\n",
    "inputDir <- Rinput[[2]]\n",
    "workDir <- Rinput[[3]]\n",
    "summMethod <- Rinput[[4]]\n",
    "minPsmsPerProtein <- Rinput[[5]]\n",
    "usePtmsForQuant <- Rinput[[6]]\n",
    "targetFdr <- Rinput[[7]]\n",
    "\n",
    "# library causes the execution to fail if the library is missing\n",
    "suppressWarnings(suppressMessages(library(lattice)))\n",
    "suppressWarnings(suppressMessages(library(stringr)))\n",
    "suppressWarnings(suppressMessages(library(mzID)))\n",
    "#suppressWarnings(suppressMessages(library(matrixStats)))\n",
    "#suppressWarnings(suppressMessages(library(venneuler)))\n",
    "suppressWarnings(suppressMessages(library(MSnbase)))\n",
    "\n",
    "# warnings as stdout\n",
    "sink(stdout(), type = \"message\")\n",
    "\n",
    "# Maybe TODO: Add option to provide mzML directly\n",
    "\n",
    "print(inputDir)\n",
    "setwd(inputDir)\n",
    "\n",
    "## Folder names for different runs (e.g. different replicates, ...)\n",
    "samples <- s <- c(\".\")\n",
    "PSMDat <- PepDat <- ProtDat <- list()\n",
    "for (s in samples) {\n",
    "  \n",
    "  ## Organize technical runs in the same folder\n",
    "  # filenames\n",
    "  ident_files <- list.files(paste(\"/home/biodocker/OUT/\",s,sep=\"\"),pattern=\"Extended_PSM_Report.txt$\",full.names=T)\n",
    "  #mzML_files <- list.files(s,pattern=\".mzML$\",full.names = T)\n",
    "  #mgf_files <- list.files(s,pattern=\"_noquote.mgf$\",full.names = T)\n",
    "  mgf_files <- list.files(s,pattern=\"mgf.filtered$\",full.names = T)\n",
    "  \n",
    "  # process the input files\n",
    "  max.fdr <- targetFdr\n",
    "  \n",
    "  # translate into labeling method descriptors\n",
    "  quant.methods <- cbind(c(\"TMT 10-plex of K,TMT 10-plex of peptide N-term\",\"TMT 6-plex of K,TMT 6-plex of peptide N-term\",\n",
    "                           \"iTRAQ 4-plex of K,iTRAQ 4-plex of Y, iTRAQ 4-plex of peptide N-term\",\n",
    "                           \"iTRAQ 8-plex of K,iTRAQ 8-plex of Y,iTRAQ 8-plex of peptide N-term\",\n",
    "                          \"iTRAQ 4-plex of K,iTRAQ 4-plex of peptide N-term\",\n",
    "                           \"iTRAQ 8-plex of K,iTRAQ 8-plex of peptide N-term\"),\n",
    "                         c(\"TMT10\",\"TMT6\",\"iTRAQ4\",\"iTRAQ8\",\"iTRAQ4\", \"iTRAQ8\"))\n",
    "    if(is.null(quant.methods[quant.methods[,1] == labeling,2])) {\n",
    "        stop(\"Error: labeling method not available\")\n",
    "    }\n",
    "  quant.method <- get(quant.methods[quant.methods[,1] == labeling,2])\n",
    "   \n",
    "    \n",
    "   ## TODO read from experimental design\n",
    "  class.labels <- c(\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \"J\")\n",
    "  args <- commandArgs(trailingOnly = TRUE)\n",
    "  \n",
    "  if (is.null(ident_files)) {\n",
    "    stop(\"Error: No identification files\")\n",
    "  }\n",
    "\n",
    "      if (is.null(mgf_files)) {\n",
    "    stop(\"Error: No spectrum files\")\n",
    "  }\n",
    "\n",
    "    \n",
    "  if (!file.exists(ident_files)) {\n",
    "    stop(\"Error: Cannot find identification files \", ident_files)\n",
    "  }\n",
    "  for (mgf_file in mgf_files) {\n",
    "    if (!file.exists(mgf_file)) {\n",
    "      stop(\"Error: Cannot find mgf file \", mgf_file)\n",
    "    }\n",
    "  }\n",
    "  \n",
    "  psms <- read.csv(ident_files, sep = \"\\t\",stringsAsFactors = F)\n",
    "  \n",
    "  if (! \"Decoy\" %in% names(psms)) {\n",
    "    stop(\"Error: No decoy information available in output file\")\n",
    "  }\n",
    "  \n",
    "  print(paste(\"Loaded\",nrow(psms), \"PSMs\"))\n",
    "  \n",
    "  # ---- Confidence filter ----\n",
    "  psms <- psms[order(psms[, \"Confidence....\"], decreasing = T), ]\n",
    "  decoy.psms <- which(psms[, \"Decoy\"] == \"1\")\n",
    "  \n",
    "  decoy.count <- 0\n",
    "  \n",
    "  for (decoy.index in decoy.psms) {\n",
    "    decoy.count <- decoy.count + 1\n",
    "    target.count <- decoy.index - decoy.count\n",
    "    \n",
    "    cur.fdr <- (decoy.count * 2) / (decoy.count + target.count)\n",
    "    \n",
    "    if (cur.fdr > max.fdr) {\n",
    "      # filter\n",
    "      psms <- psms[1:decoy.index - 1,]\n",
    "      break\n",
    "    }\n",
    "  }\n",
    "  \n",
    "print(paste0(\"Filtered \", nrow(psms), \" PSMs @ \", max.fdr, \" FDR\"))\n",
    "    \n",
    "  if (nrow(psms) < 1) {\n",
    "      stop(\"Error: No valid PSMs found\")\n",
    "  }\n",
    "    \n",
    "    #print(head(psms))\n",
    "    \n",
    "  #### prepare for MSnbase\n",
    "  psms$rank <- 1\n",
    "  psms$desc <- psms$Protein.s.\n",
    "  psms$spectrumID <- psms$Spectrum.Title #str_extract(psms$Spectrum.Title, \"scan=[0-9]*\")\n",
    "  psms$spectrumFile <- psms$Spectrum.File\n",
    "  psms$idFile <- ident_files\n",
    "  # remove unnecessary PTMs from modified sequence\n",
    "  # TODO: define whether to take oxidation, ...\n",
    "  psms$Modified.Sequence <- gsub(\"<cmm>\",\"\",psms$Modified.Sequence)\n",
    "  psms$Modified.Sequence <- sub(\"[a-z,A-Z]*-\",\"\",psms$Modified.Sequence)\n",
    "  psms$Modified.Sequence <- sub(\"-[a-z,A-Z]*\",\"\",psms$Modified.Sequence)\n",
    "  psms$Modified.Sequence <- gsub(\"<iTRAQ>\",\"\",psms$Modified.Sequence)\n",
    "  psms$Modified.Sequence <- gsub(\"<TMT>\",\"\",psms$Modified.Sequence)\n",
    "  psms$sequence <- psms$Modified.Sequence\n",
    "    #Optional:\n",
    "  # psms$Modified.Sequence <- gsub(\"<ox>\",\"\",psms$Modified.Sequence)\n",
    "  \n",
    "  \n",
    "  \n",
    "  # reading MS and identification data\n",
    "  allSpectra <- list()\n",
    "  for (mgf_file in mgf_files) {\n",
    "    # myExp1 <- readMSData(mzML_files, mode=\"onDisk\", verbose=T)\n",
    "    \n",
    "    myExp1 <- readMgfData(mgf_file, verbose=T)\n",
    "    # myExp1 <- MSnbase::readMgfData(\"t.mgf\", verbose=T)\n",
    "    for (i in 1:ncol(fData(myExp1))) {\n",
    "      if (is.factor(fData(myExp1)[,i]))\n",
    "        fData(myExp1)[,i] <- as.character(fData(myExp1)[,i])\n",
    "    }\n",
    "    fData(myExp1)$spectrumFile <- mgf_file\n",
    "    allSpectra[[mgf_file]] <- myExp1 #updateFeatureNames(myExp1,label = paste(\"Sample\",i))\n",
    "\n",
    "  }\n",
    "\n",
    "  print(\"Adding identifications ...\")\n",
    "  \n",
    "  # myExp1 <- addIdentificationData(myExp1,psms,decoy=\"Decoy\",rank=\"rank\",acc=\"Protein.s.\",\n",
    "  #                                 icol=\"spectrumID\",fcol=\"spectrumId\",desc=\"desc\",pepseq=\"Modified.Sequence\",verbose=T)\n",
    "  cl <- makeCluster(NumThreads)\n",
    "  myExp <- qnt <- list()\n",
    "  for (mgf_file in mgf_files) {\n",
    "    myExp[[mgf_file]] <- addIdentificationData(allSpectra[[mgf_file]],psms,decoy=\"Decoy\",rank=\"rank\",acc=\"Protein.s.\",\n",
    "                                  icol=\"Spectrum.Title\",fcol=\"TITLE\",desc=\"desc\",pepseq=\"Modified.Sequence\",verbose=T)\n",
    "    print(idSummary(myExp[[mgf_file]]))\n",
    "    myExp[[mgf_file]] <- removeNoId(myExp[[mgf_file]])\n",
    "    qnt[[mgf_file]] <- quantify(myExp[[mgf_file]], method=\"sum\", reporters = quant.method, strict =F, verbose=T )\n",
    "      \n",
    "          # Plot reporter QC\n",
    "    hist(unlist(mz(myExp[[mgf_file]])), 1000, main=paste(\"m/z accuracy of reporter ions\\n\",mgf_file), \n",
    "         xlim=range(quant.method@mz)+c(-1,1),xlab=\"m/z\",col=\"#666666\",border=NA)\n",
    "    abline(v=quant.method@mz,col=2,lwd=0.5)\n",
    "\n",
    "  } \n",
    "\n",
    "  ## Convert SearchGUI output to file format that can be processed by isobar\n",
    "  stopCluster(cl)\n",
    "\n",
    "print(\"Adding reporters ...\")    \n",
    "    \n",
    "  imp<-makeImpuritiesMatrix(length(quant.method),edit=F)\n",
    "  for (i in 1:length(qnt)) {\n",
    "    qnt[[i]] <- purityCorrect(qnt[[i]],imp)\n",
    "    exprs(qnt[[i]]) <- log2(exprs(qnt[[i]]))\n",
    "    qnt[[i]] <- normalise(qnt[[i]],\"center.median\")\n",
    "    qnt[[i]] <- updateFeatureNames(qnt[[i]],label = paste(\"Sample\",i))\n",
    "  }\n",
    "      \n",
    "  names(qnt) <- NULL\n",
    "  allqnt <- do.call(\"combine\",args=qnt)\n",
    "  allqnt <- filterNA(allqnt, pNA=0.5)\n",
    "\n",
    "# ---- add the sample annotations ----\n",
    "  # load experimental design description    \n",
    "  exp.design <- read.csv(paste0(workDir, \"/\", \"exp_design.tsv\"), sep = \"\\t\")\n",
    "    \n",
    "  if (nrow(exp.design) != nrow(pData(allqnt))) {\n",
    "    stop(\"Error: Experimental design does not fit the number of quantified ions.\")\n",
    "  }\n",
    "\n",
    "  # merge the annotations\n",
    "  pdata.org <- pData(allqnt)\n",
    "  pdata.combined <- merge(pdata.org, exp.design, all.x = T, all.y = F, by.x = 0, by.y = \"channel\")\n",
    "  rownames(pdata.combined) <- pdata.combined[, \"Row.names\"]\n",
    "  pdata.combined$Row.names <- NULL\n",
    "\n",
    "  # save\n",
    "  pData(allqnt) <- pdata.combined[colnames(allqnt), ]\n",
    "  \n",
    "  ## Setting the stage for the iPQF inference method\n",
    "  names(fData(allqnt))[which(names(fData(allqnt))==\"Protein.s.\")] <- \"accession\"\n",
    "  names(fData(allqnt))[which(names(fData(allqnt))==\"Variable.Modifications\")] <- \"modifications\"\n",
    "  names(fData(allqnt))[which(names(fData(allqnt))==\"m.z\")] <- \"mass_to_charge\"\n",
    "  names(fData(allqnt))[which(names(fData(allqnt))==\"Confidence....\")] <- \"search_engine_score\"\n",
    "    \n",
    "  write.csv(exprs(allqnt),paste(\"/home/biodocker/OUT/\",s,\"/AllQuantPSMs.csv\",sep=\"\"))  \n",
    "    save(allqnt, file=paste(\"/home/biodocker/OUT/\",s,\"/AllQuantPSMs.RData\",sep=\"\") )\n",
    "    \n",
    "    PSMDat[[s]] <- allqnt\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "}    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-16T17:26:07.215902Z",
     "start_time": "2018-05-16T17:26:05.236572Z"
    },
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "%%R --width 800 --height 800\n",
    "\n",
    "\n",
    "library(plotly)\n",
    "library(reshape)\n",
    "\n",
    "\n",
    "# TODO read experimental design\n",
    "ExpDesign <- read.table(\"/home/biodocker/OUT/exp_design.tsv\")\n",
    "\n",
    "\n",
    "panel.cor <- function(x, y, digits=2, prefix=\"\", cex.cor, ...) \n",
    "{\n",
    "    usr <- par(\"usr\"); on.exit(par(usr)) \n",
    "    par(usr = c(0, 1, 0, 1)) \n",
    "    r <- abs(cor(x, y, use=\"na.or.complete\")) \n",
    "    txt <- format(c(r, 0.123456789), digits=digits)[1] \n",
    "    txt <- paste(prefix, txt, sep=\"\") \n",
    "    cex.cor <- 0.8/strwidth(txt) \n",
    "    test <- cor.test(x,y) \n",
    "    # borrowed from printCoefmat\n",
    "    Signif <- symnum(test$p.value, corr = FALSE, na = FALSE, \n",
    "                  cutpoints = c(0, 0.001, 0.01, 0.05, 0.1, 1),\n",
    "                  symbols = c(\"***\", \"**\", \"*\", \".\", \" \")) \n",
    " \n",
    "    text(0.5, 0.5, txt, cex = cex.cor * r) \n",
    "   # text(.8, .8, Signif, cex=cex.cor, col=2) \n",
    "}\n",
    "\n",
    "panel.hist <- function(x, hist.col=\"#993333\", ...)\n",
    "{\n",
    "    usr <- par(\"usr\"); on.exit(par(usr))\n",
    "   par(usr = c(usr[1:2], 0, 1.5) )\n",
    "    h <- hist(x, plot = FALSE, border=NA,breaks=50)\n",
    "    breaks <- h$breaks; nB <- length(breaks)\n",
    "  y <- h$counts; y <- y/max(y)\n",
    "    rect(breaks[-nB], 0, breaks[-1], y,col=hist.col,border=NA)\n",
    "}\n",
    "\n",
    "for (s in samples) {\n",
    "    allqnt <- PSMDat[[s]]\n",
    "    \n",
    "    condNames <- ExpDesign[ExpDesign[,2] == unique(ExpDesign[,2])[which(s==samples)],1]\n",
    "    conditions <- condNames\n",
    "    condNames <- paste(condNames, sampleNames(allqnt), sep=\"\\n\")\n",
    "    pData(allqnt)$sample_name <- paste(condNames, sampleNames(allqnt))\n",
    "pData(allqnt)$sample_group <- condNames\n",
    "    \n",
    "    \n",
    "    #print(pData(allqnt))\n",
    "    \n",
    "    ## QC Plots\n",
    "  #boxplot(exprs(allqnt), main =paste(\"Sample\",s), labels =  condNames)\n",
    "    \n",
    "       conditionsPlot <- rep(conditions, each=nrow(exprs(allqnt)))\n",
    "    p <- ggplot(melt(exprs(allqnt)), aes(x=X2, y=value, fill=conditionsPlot)) + geom_violin(trim=FALSE) + \n",
    "  geom_boxplot(width=0.1) + theme_classic() + theme(axis.text.x = element_text(angle = 90, hjust = 1)) +\n",
    "    xlab(\"\") + ylab(\"PSM expression\")\n",
    "\n",
    "print(p)\n",
    "        \n",
    "  pairs(exprs(allqnt),lower.panel=panel.smooth, upper.panel=panel.cor, diag.panel=panel.hist, main = paste(\"Sample\",s),\n",
    "cex=0.1,col=\"#33333388\",pch=15, labels =  condNames)\n",
    "    \n",
    "  #print(fData(allqnt))  \n",
    "}\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-16T17:26:07.463252Z",
     "start_time": "2018-05-16T17:26:07.218647Z"
    },
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "%%R --width 800 --height 800\n",
    "\n",
    "print(\"PSM filtering and impurity correction\")\n",
    "\n",
    "for (s in samples) {\n",
    "    allqnt <- PSMDat[[s]]\n",
    "\n",
    "imp<-makeImpuritiesMatrix(length(quant.method),edit=F)\n",
    "  for (i in 1:length(qnt)) {\n",
    "    qnt[[i]] <- purityCorrect(qnt[[i]],imp)\n",
    "    exprs(qnt[[i]]) <- log2(exprs(qnt[[i]]))\n",
    "    qnt[[i]] <- normalise(qnt[[i]],\"center.median\")\n",
    "    qnt[[i]] <- updateFeatureNames(qnt[[i]],label = paste(\"Sample\",i))\n",
    "  }\n",
    "      \n",
    "  names(qnt) <- NULL\n",
    "  allqnt <- do.call(\"combine\",args=qnt)\n",
    "      \n",
    "  allqnt <- filterNA(allqnt, pNA=0.5)\n",
    "\n",
    "\n",
    "  ## Setting the stage for the iPQF inference method\n",
    "  names(fData(allqnt))[which(names(fData(allqnt))==\"Protein.s.\")] <- \"accession\"\n",
    "  names(fData(allqnt))[which(names(fData(allqnt))==\"Variable.Modifications\")] <- \"modifications\"\n",
    "  names(fData(allqnt))[which(names(fData(allqnt))==\"m.z\")] <- \"mass_to_charge\"\n",
    "  names(fData(allqnt))[which(names(fData(allqnt))==\"Confidence....\")] <- \"search_engine_score\"\n",
    "    head(allqnt)\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-16T17:26:08.607448Z",
     "start_time": "2018-05-16T17:26:07.465427Z"
    },
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "%%R --width 800 --height 800\n",
    "\n",
    "print(\"Protein inference ...\")    \n",
    "\n",
    "for (s in samples) {\n",
    "\n",
    "    load(paste(\"/home/biodocker/OUT/\",s,\"/AllQuantPSMs.RData\",sep=\"\"))\n",
    "exprs(allqnt) <- 2^exprs(allqnt)\n",
    "\n",
    "print(paste(\"Sample\",s))\n",
    "    \n",
    "    \n",
    "  #Problem: iPQF does not seem to work with missing values!\n",
    "  allProts <- combineFeatures(allqnt, groupBy=fData(allqnt)$accession, fun=\"medpolish\",verbose=F, na.rm=T)\n",
    " exprs(allProts) <- log2(exprs(allProts))\n",
    "   \n",
    "    \n",
    "  #print(head(melt(exprs(allProts))))\n",
    "    \n",
    "  #boxplot(exprs(allProts\n",
    "    \n",
    "    p <- ggplot(melt(exprs(allProts)), aes(x=X2, y=value, fill=rep(conditions,each=nrow(exprs(allProts))))) + geom_violin(trim=FALSE) + \n",
    "  geom_boxplot(width=0.1) + theme_classic() + theme(axis.text.x = element_text(angle = 90, hjust = 1)) +\n",
    "    xlab(\"\") + ylab(\"Protein expression\")\n",
    "\n",
    "print(p)\n",
    "\n",
    "    \n",
    "  #stop(\"\")\n",
    "  #pairs(exprs(allProts))\n",
    "  #save\n",
    "  write.exprs(allProts,file=paste(\"/home/biodocker/OUT/\",s,\"/AllQuantProteins.csv\",sep=\"\"))\n",
    "  \n",
    "  ProtDat[[s]] <- allProts <- exprs(allProts)\n",
    "    colnames(ProtDat[[s]]) <- condNames\n",
    "  \n",
    "}\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-16T17:26:08.623402Z",
     "start_time": "2018-05-16T17:26:08.609641Z"
    },
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "%%R --width 1000 --height 600\n",
    "\n",
    "print(\"Merging samples (if needed) ...\")\n",
    "\n",
    "par(mfrow=c(1,1))\n",
    "\n",
    "\n",
    "# Merge different samples\n",
    "    totProts <- ProtDat[[1]]\n",
    "if (length(samples)>1) {\n",
    "    for (s in samples[2:length(samples)])\n",
    "    totProts <- merge(totProts,ProtDat[[s]], all=T, by.x=1, by.y=0)\n",
    "} \n",
    "\n",
    "write.csv(totProts,file=paste(\"/home/biodocker/OUT/AllQuantProteinsInAllSamples.csv\",sep=\"\"))\n",
    "\n",
    "# PCA\n",
    "  pca <- princomp(totProts[complete.cases((totProts)),])\n",
    "  #plot(pca)\n",
    "  plot(pca$loadings, cex=2, pch=16, col=as.numeric(as.factor(ExpDesign[,1])))\n",
    "  text(pca$loadings,colnames(totProts), pos=2)\n",
    "    \n",
    "  print(paste(\"Quantified a total of\",nrow(allProts),\"protein groups\"))\n",
    "\n",
    "\n",
    "\n",
    "##Statistics\n",
    "library(limma)\n",
    "library(qvalue)\n",
    "NumCond <- length(unique(ExpDesign[,1]))\n",
    "  if (NumCond < 2)\n",
    "      stop(\"Only 1 experimental condition -> no statistics\")\n",
    "\n",
    "design <- model.matrix(~0+factor(ExpDesign[,1])-1)\n",
    "  colnames(design)<-paste(unique(ExpDesign[,1]),i,sep=\"\")\n",
    "  contrasts<-NULL\n",
    "  First <- 1\n",
    "  for (i in (1:NumCond)[-First]) contrasts<-append(contrasts,paste(colnames(design)[i],\"-\",colnames(design)[First],sep=\"\"))\n",
    "  contrast.matrix<-makeContrasts(contrasts=contrasts,levels=design)\n",
    "  # print(dim(Data))\n",
    "  lm.fitted <- lmFit(totProts,design)\n",
    "  lm.contr <- contrasts.fit(lm.fitted,contrast.matrix)\n",
    "  lm.bayes<-eBayes(lm.contr)\n",
    "  topTable(lm.bayes)\n",
    " plvalues <- lm.bayes$p.value\n",
    "fcs <- lm.bayes$coefficients\n",
    "  qlvalues <- matrix(NA,nrow=nrow(plvalues),ncol=ncol(plvalues),dimnames=dimnames(plvalues))\n",
    "  # qvalue correction\n",
    " for (i in 1:ncol(plvalues)) {\n",
    "    tqs <- qvalue(na.omit(plvalues[,i]))$qvalues\n",
    "    qlvalues[names(tqs),i] <- tqs\n",
    "  }\n",
    "  \n",
    "\n",
    "par(mfrow=c(1,3))\n",
    "\n",
    "statTable <- NULL\n",
    "\n",
    "# Visualizations: volcano plot, number of regulated proteins per FDR, interactive table?\n",
    "for (i in 1:ncol(plvalues)) {\n",
    "    hist(plvalues[,i],100,border=NA,col=\"#555555\", main=paste(\"p-value distribution\\nComparison\",colnames(fcs)[i], sep=\"\\n\"),\n",
    "        xlab=\"p-values\")\n",
    "    plot(fcs[,i], -log10(qlvalues[,i]),pch=16,col=\"#44FFFF99\",\n",
    "        xlab=\"log(fold-change)\", ylab=\"-log10(FDR)\", main=paste(\"Volcano plot\\nComparison\",\n",
    "                                                                colnames(fcs)[i], sep=\"\\n\"),\n",
    "        ylim=c(0,max(2,max(-log10(qlvalues[,i])))))\n",
    "    abline(h=-log10(c(0.001,0.01,0.05)), col=2:4,lwd=2)\n",
    "    text(c(0,0,0), c(3,2,1.3),c(\"FDR=0.001\",\"FDR=0.01\",\"FDR=0.05\"),pos=1)\n",
    "    ddd <- c(0.0001,sort(qlvalues[,i]))\n",
    "plot(ddd, 0:(length(ddd)-1),type=\"l\",xlim=c(1e-3,1),log=\"x\", main=paste(\"Significant protein versus FDR\\nComparison\",colnames(fcs)[i], sep=\"\\n\")) \n",
    "    abline(v=c(0.001,0.01,0.05), col=2:4, xlab=\"FDR\", ylab=\"Number of proteins\")\n",
    "    \n",
    "}\n",
    "\n",
    "# How far should we go? Clustering (when having more then 2 groups)? \n",
    "# Hierarchical clustering of significant features? clusterProfiler?\n",
    "\n",
    "\n",
    "# Saving stats\n",
    "statOut <- cbind(fcs, plvalues, qlvalues)\n",
    "colnames(statOut) <- paste(rep(c(\"log(fold-change)\",\"p-values\",\"q-values\"), each=ncol(plvalues)), colnames(statOut))\n",
    "statOut <- cbind(Proteins=rownames(fcs),statOut)\n",
    "print(head(statOut))\n",
    "write.csv(statOut, \"/home/biodocker/OUT/DifferentiallyRegulatedProteins.csv\",row.names=F)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-10T14:53:52.861990Z",
     "start_time": "2018-05-10T14:53:52.092468Z"
    },
    "hideCode": false,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "# test ground for interactive apps\n",
    "library(plotly)\n",
    "library(reshape)\n",
    "\n",
    "#print(head(melt(allProts)))\n",
    "\n",
    "x <- rnorm(1000)\n",
    "y <- rchisq(1000, df = 1, ncp = 0)\n",
    "group <- sample(LETTERS[1:5], size = 1000, replace = T)\n",
    "size <- sample(1:5, size = 1000, replace = T)\n",
    "\n",
    "ds <- data.frame(x, y, group, size)\n",
    "\n",
    "p <- plot_ly(ds, x = x, y = y, mode = \"markers\", split = group, size = size) %>%\n",
    "  layout(title = \"Scatter Plot\")\n",
    "embed_notebook(p)\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-10T08:59:59.347465Z",
     "start_time": "2018-05-10T08:59:59.335751Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-10T14:38:27.204016Z",
     "start_time": "2018-05-10T14:38:27.136782Z"
    }
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "ddd <- sort(qlvalues[,1])\n",
    "plot(ddd, 1:length(ddd),type=\"l\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-10T09:14:03.702860Z",
     "start_time": "2018-05-10T09:14:03.696724Z"
    }
   },
   "outputs": [],
   "source": [
    "display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Hide code",
  "hide_code_all_hidden": true,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "notify_time": "5"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
