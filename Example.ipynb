{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "# Protocol for analysis of labeled proteomics data\n",
    "We recommend the following structure for describing the protocol and its example workflow. Below you can find an overview of required features of the protocol and its description and rules for best practices.\n",
    "\n",
    "This is the link to the github repository of the protocol \n",
    "TODO add version!\n",
    "https://github.com/ProtProtocols/biocontainer-jupyter\n",
    "\n",
    "Link to docker image:\n",
    "\n",
    "\n",
    "\n",
    "## Abstract\n",
    "Provide a short description of the software protocol including broader context, functionality, use case and purpose. \n",
    "\n",
    "\n",
    "## Maintainer\n",
    "Provide details about the protocol maintainer (e.g. email address and/or github username)\n",
    "\n",
    "## Software\n",
    "Specify links for documentation and tutorials of used software, source code, publications and use cases. Detail versions of each used software. Alternatively, provide links to the software descriptions in https://bio.tools where this information is available.\n",
    "\n",
    "## Diagram\n",
    "Provide a simple diagram of functionality of the workflow/software. We recommend using controlled vocabularies for input/output data types and file formats as well as provided operation of the tool(s). You can use http://edamontology.org terms for the description.\n",
    "\n",
    "__TODO: example__\n",
    "\n",
    "## System requirements\n",
    "Fill in the following items:\n",
    "Required hard disk space for docker image, input and output files: \n",
    "\n",
    "Required memory: \n",
    "\n",
    "Recommmended number of threads: \n",
    "\n",
    "## Example \n",
    "Presentation of well-documented instructions and commands to run the example use case. Depending on the use case and the software, provide link(s) to open the web service incorportated in the Docker image (e.g. 0.0.0.0:8080), bash commands to run programs from the command line and additional code for e.g. checking and visualizing the (intermediate) results. \n",
    "\n",
    "Instead of providing the instructions in this notebook, one can also provide a link to a notebook containing the example use case.\n",
    "\n",
    "## More general use case (optional)\n",
    "Provide link to notebook with a generalized use case that easily can be adapted to e.g. process different input data and concurrent parametrization.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "# Example\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO list\n",
    "\n",
    "- Remove quotes from titles in mgf-files\n",
    "- Add possibility of samples that are arranged in different folders\n",
    "- Python script: Adapt all MGF titles prior to search\n",
    "- Python script: Fix search with new PeptideShaker version\n",
    "- Python script: Add MGF Peak filter function (100 - 150)\n",
    "- Evaluate TMTc: https://pubs.acs.org/doi/10.1021/acs.analchem.7b04713\n",
    "- Protein inference function\n",
    "- Protein summarization function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "Specify parameters for database search and evaluation of identified peptide-spectrum matches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-30T19:18:04.001216Z",
     "start_time": "2018-03-30T19:18:03.508815Z"
    },
    "hideCode": true,
    "hideOutput": true,
    "hidePrompt": false,
    "hide_input": true,
    "init_cell": true,
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "%load_ext rpy2.ipython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-30T19:18:07.012675Z",
     "start_time": "2018-03-30T19:18:04.003892Z"
    },
    "code_folding": [
     52,
     152,
     313
    ],
    "hideCode": true,
    "hidePrompt": false,
    "hide_input": true,
    "init_cell": true,
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from ipywidgets import VBox, Label\n",
    "import sys, os\n",
    "import subprocess\n",
    "\n",
    "# create an empty class as a storage for all UI widgets\n",
    "class SearchUI:\n",
    "    def __init__(self):\n",
    "        self.work_dir_select = widgets.Dropdown(options={'/data/': '/data/', 'Example files': '/home/biodocker/'}, \n",
    "                                                value='/home/biodocker/')\n",
    "        self.work_dir_select.observe(observe_work_dir_select)\n",
    "        \n",
    "        self.precursor_tolerance = widgets.IntSlider(min=-10,max=30,step=1,value=20)\n",
    "        self.fragment_tolerance = widgets.BoundedFloatText(min=0,max=200,value=0.05)\n",
    "        self.fasta_db = widgets.Dropdown(options={\"sp_human.fasta\": \"IN/sp_human.fasta\"})\n",
    "        \n",
    "        self.generate_decoy = widgets.Checkbox(value=True, description=\"Generate decoy sequences\")\n",
    "        \n",
    "        # TODO  needs table to describe labeling formats\n",
    "        self.labelling = widgets.Dropdown(options=\n",
    "                          {'TMT6': 'TMT 6-plex of K,TMT 6-plex of peptide N-term',\n",
    "                           'TMT10': 'TMT 10-plex of K,TMT 10-plex of peptide N-term','iTRAQ4': 'iTRAQ 4-plex of K,iTRAQ 4-plex of Y,iTRAQ 4-plex of peptide N-term',\n",
    "                           'iTRAQ8 (fixed)': 'iTRAQ 8-plex of K, iTRAQ 8-plex of peptide N-term',\n",
    "                           'iTRAQ8 (variable)': 'iTRAQ 8-plex of Y'},\n",
    "                      value='TMT 10-plex of K,TMT 10-plex of peptide N-term')\n",
    "        \n",
    "        self.missed_cleavages = widgets.IntSlider(min=0,max=10,step=1,value=1)\n",
    "        self.fixed_ptms = widgets.Dropdown(options=[\"Carbamidomethylation of C\",\"None\"])\n",
    "\n",
    "        # PTMs\n",
    "        self.var_ptms = widgets.SelectMultiple(\n",
    "            options=[\"Oxidation of M\",\n",
    "                     \"Phosphorylation of STY\",\n",
    "                     \"Acetylation of peptide N-term\",\n",
    "                     \"Acetylation of protein N-term\"],\n",
    "            value=['Oxidation of M'])\n",
    "        \n",
    "        self.spectra_dir = widgets.Dropdown(options={\"IN\": \"IN\"})\n",
    "\n",
    "        # ww = widgets.Checkbox(description=\"Decoy\")\n",
    "\n",
    "        self.search_button = widgets.Button(\n",
    "            description='Run Search',\n",
    "            disabled=False,\n",
    "            button_style='', # 'success', 'info', 'warning', 'danger' or ''\n",
    "            tooltip='Launch the search',\n",
    "            icon='check'\n",
    "        )\n",
    "\n",
    "        self.search_button.on_click(run_search)\n",
    "        \n",
    "        \n",
    "    def updateFastaFiles(self, workdir):\n",
    "        # get all FASTA files\n",
    "        fasta_files = [file for file in os.listdir(workdir) if file[-6:] == \".fasta\"]\n",
    "        \n",
    "        # also search all subdirectories for FASTA files\n",
    "        for d in os.listdir(workdir):\n",
    "            d_path = os.path.join(workdir, d)\n",
    "            if os.path.isdir(d_path) and d[0] != \".\":\n",
    "                fasta_files += [os.path.join(d, file) for file in os.listdir(d_path) if file[-6:] == \".fasta\"]\n",
    "        \n",
    "        # create the dict to add as values to the control\n",
    "        file_list = dict()\n",
    "        sel_value = None\n",
    "        \n",
    "        for f in fasta_files:\n",
    "            file_list[f] = os.path.join(os.path.abspath(workdir), f)\n",
    "            if sel_value is None:\n",
    "                sel_value = os.path.join(os.path.abspath(workdir), f)\n",
    "        \n",
    "        self.fasta_db.options = file_list\n",
    "        self.fasta_db.value = sel_value\n",
    "        \n",
    "        # update the list of possible peaklist directories\n",
    "        directories = [d for d in os.listdir(workdir) if os.path.isdir(os.path.join(workdir, d)) and d[0] != \".\"]\n",
    "        \n",
    "        dir_list = dict()\n",
    "        \n",
    "        for d in directories:\n",
    "            dir_list[d] = os.path.join(os.path.abspath(workdir), d)\n",
    "            \n",
    "        self.spectra_dir.options = dir_list\n",
    "        \n",
    "        if \"IN\" in dir_list:\n",
    "            self.spectra_dir.value = dir_list[\"IN\"]\n",
    "        \n",
    "        \n",
    "    def display(self):\n",
    "        self.updateFastaFiles(self.work_dir_select.value)\n",
    "        \n",
    "        settings_box = VBox([Label('Working directory'), self.work_dir_select,\n",
    "                             Label('Precursor tolerance (ppm):'), self.precursor_tolerance, \n",
    "                             Label('Fragment ion tolerance (da):'), self.fragment_tolerance,\n",
    "                             Label('Fasta file (database, must NOT contain decoy sequences):'), self.fasta_db,\n",
    "                             self.generate_decoy,\n",
    "                             Label('Quantification method:'), self.labelling,\n",
    "                             Label('Number of miscleavages;'), self.missed_cleavages,\n",
    "                             Label('Further fixed modifications'), self.fixed_ptms,\n",
    "                             Label('Further variable modifications (Hold Ctrl to select multiple)'), self.var_ptms,\n",
    "                             Label('Folder for spectra files (files need to be mgf)'), self.spectra_dir,\n",
    "                             self.search_button])\n",
    "\n",
    "        display(settings_box)\n",
    "        \n",
    "\n",
    "class ExpDesignUI:\n",
    "    def __init__(self, labelling_technique):\n",
    "        \"\"\"\n",
    "        Generates all use interface objects as member variables.\n",
    "        \n",
    "        :param labelling_technique: The labelling method used.\n",
    "        \"\"\"\n",
    "        # always expect two groups\n",
    "        self.group1_name = widgets.Text(placeholder = \"Treatment\", description = \"Group 1:\")\n",
    "        self.group2_name = widgets.Text(placeholder = \"Control\", description = \"Group 2:\")\n",
    "        \n",
    "        self.channels = {\n",
    "            'TMT6': [\"126\", \"127\", \"128\", \"129\", \"130\", \"131\"],\n",
    "            'TMT10': [\"126\", \"127N\", \"127C\", \"128N\", \"128C\", \"129N\", \"129C\", \"130N\", \"130C\", \"131\"],\n",
    "            'iTRAQ8': [\"113\", \"114\", \"115\", \"116\", \"117\", \"118\", \"119\", \"121\"]\n",
    "        }\n",
    "        \n",
    "        if labelling_technique not in self.channels:\n",
    "            raise Exception(\"Unknown labelling technique: '\" + labelling_technique + \"'\")\n",
    "            \n",
    "        self.labelling_technique = labelling_technique\n",
    "            \n",
    "        # generate the textfields for the channels\n",
    "        self.channel_names = list()\n",
    "        \n",
    "        for channel in self.channels[self.labelling_technique]:\n",
    "            self.channel_names.append(widgets.Text(description = channel, placeholder = \"Sample \" + channel))\n",
    "            \n",
    "        # add select boxes to select the experimental group\n",
    "        self.group_selects = list()\n",
    "        \n",
    "        for channel in self.channels[self.labelling_technique]:\n",
    "            self.group_selects.append(widgets.Dropdown(options = [\"Group 1\", \"Group 2\"], value = \"Group 1\"))\n",
    "            \n",
    "        self.save_button = widgets.Button(\n",
    "            description='Save design',\n",
    "            disabled=False,\n",
    "            button_style='', # 'success', 'info', 'warning', 'danger' or ''\n",
    "            tooltip='Save the experimental design',\n",
    "            icon='check'\n",
    "        )\n",
    "\n",
    "        self.save_button.on_click(self.save_design)\n",
    "\n",
    "    def display(self):\n",
    "        widget_list = [widgets.Label(\"Treatment group names:\"), self.group1_name, self.group2_name,\n",
    "                       widgets.Label(\"Sample names (per channel):\")]\n",
    "        \n",
    "        for i in range(0, len(self.channel_names)):\n",
    "            widget_list.append(widgets.HBox([self.channel_names[i], self.group_selects[i]]))\n",
    "            \n",
    "        widget_list.append(self.save_button)\n",
    "        \n",
    "        widget_box = VBox(widget_list)\n",
    "        \n",
    "        display(widget_box)\n",
    "        \n",
    "    def save_design(self):\n",
    "        pass\n",
    "            \n",
    "        \n",
    "def run_search(button):\n",
    "    global searchUI, result_file\n",
    "    \n",
    "    # make sure all required fields were selected\n",
    "    if searchUI.work_dir_select.value is None:\n",
    "        print(\"Error: No working directory selected\")\n",
    "        return\n",
    "    \n",
    "    if searchUI.fasta_db.value is None:\n",
    "        print(\"Error: No FASTA file selected\")\n",
    "        return\n",
    "    \n",
    "    # create the directory paths to work in\n",
    "    peaklist_dir = os.path.abspath(searchUI.spectra_dir.value)\n",
    "    \n",
    "    if not os.path.isdir(peaklist_dir):\n",
    "        raise Exception(\"Invalid peak list directory selected: \" + peaklist_dir + \" does not exist.\")\n",
    "    \n",
    "    peptide_shaker_jar = \"/home/biodocker/bin/PeptideShaker-1.16.17/PeptideShaker-1.16.17.jar\"\n",
    "    searchgui_jar = \"/home/biodocker/bin/SearchGUI-3.2.20/SearchGUI-3.2.20.jar\"\n",
    "    work_dir = os.path.abspath(os.path.join(searchUI.work_dir_select.value, \"OUT\"))\n",
    "    \n",
    "    # the searches should be performed in the \"OUT\" directory\n",
    "    if not os.path.isdir(work_dir):\n",
    "        os.mkdir(work_dir)\n",
    "        \n",
    "    # -------------------------------------\n",
    "    # Generate the decoy database\n",
    "    \n",
    "    if searchUI.generate_decoy.value == True:\n",
    "        print(\"Creating decoy database...\")\n",
    "\n",
    "        # create the decoy database\n",
    "        subprocess.run([\"java\", \"-cp\", searchgui_jar, \n",
    "                       \"eu.isas.searchgui.cmd.FastaCLI\", \"-in\", searchUI.fasta_db.value, \"-decoy\"], check=True,\n",
    "                       cwd=work_dir,\n",
    "                       stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "\n",
    "        # get the filename of the decoy database\n",
    "        database_file = os.path.abspath(searchUI.fasta_db.value)[:-6] + \"_concatenated_target_decoy.fasta\"\n",
    "    else:\n",
    "        # simply use the selected database file\n",
    "        database_file = os.path.abspath(searchUI.fasta_db.value)\n",
    "    \n",
    "    if not os.path.isfile(database_file):\n",
    "        raise Exception(\"Failed to find generated decoy database\")\n",
    "        \n",
    "    # ---------------------------------------------\n",
    "    # Create the search parameter file\n",
    "        \n",
    "    # build the arguments to create the parameter file\n",
    "    param_file = os.path.join(work_dir, \"search.par\")\n",
    "    \n",
    "    # remove any old parameters\n",
    "    if os.path.isfile(param_file):\n",
    "        os.remove(param_file)\n",
    "    \n",
    "    search_args = [\"java\", \"-cp\", searchgui_jar,\n",
    "                   \"eu.isas.searchgui.cmd.IdentificationParametersCLI\",\n",
    "                   \"-out\", param_file]\n",
    "    \n",
    "    # precursor tolerance\n",
    "    search_args.append(\"-prec_tol\")\n",
    "    search_args.append(str(searchUI.precursor_tolerance.value))\n",
    "    # fragment tolerance\n",
    "    search_args.append(\"-frag_tol\")\n",
    "    search_args.append(str(searchUI.fragment_tolerance.value))\n",
    "    # fixed mods\n",
    "    # TODO: labelling cannot always be set as fixed mod???\n",
    "    fixed_mod_string = str(searchUI.labelling.value) + \",\" + str(searchUI.fixed_ptms.value)\n",
    "    search_args.append(\"-fixed_mods\")\n",
    "    search_args.append(fixed_mod_string)\n",
    "    # database\n",
    "    search_args.append(\"-db\")\n",
    "    search_args.append(database_file)\n",
    "    # missed cleavages\n",
    "    search_args.append(\"-mc\")\n",
    "    search_args.append(str(searchUI.missed_cleavages.value))\n",
    "    \n",
    "    # var mods\n",
    "    if len(searchUI.var_ptms.value) > 0:\n",
    "        search_args.append(\"-variable_mods\")\n",
    "        var_mod_list = list()\n",
    "        \n",
    "        for var_mod in searchUI.var_ptms.value:\n",
    "            if var_mod == \"Phosphorylation of STY\":\n",
    "                var_mod_list += [\"Phosphorylation of S\", \"Phosphorylation of T\", \"Phosphorylation of Y\"]\n",
    "            else:\n",
    "                var_mod_list.append(var_mod)\n",
    "                \n",
    "        search_args.append(\",\".join(var_mod_list))\n",
    "        \n",
    "    # create the search parameter file\n",
    "    print(\"Creating search parameter file...\")\n",
    "    # print(\" \".join(search_args))\n",
    "    subprocess.run(search_args, check=True, cwd=work_dir, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    \n",
    "    if not os.path.isfile(param_file):\n",
    "        raise Exception(\"Failed to create search parameters\")\n",
    "        \n",
    "    # ------------------------------------------------\n",
    "    # Run the search\n",
    "    print(\"Running search...\")\n",
    "    # TODO: create list of spectrum files - or the folder\n",
    "    spectrum_files = peaklist_dir\n",
    "    print(\"  Searching files in \" + spectrum_files)\n",
    "    search_process = subprocess.run([\"java\", \"-cp\", searchgui_jar,\n",
    "                    \"eu.isas.searchgui.cmd.SearchCLI\", \"-spectrum_files\", spectrum_files,\n",
    "                    \"-output_folder\", work_dir, \"-id_params\", param_file,\n",
    "                    \"-xtandem\", \"0\", \"-msgf\", \"1\", \"-comet\", \"0\", \"-ms_amanda\", \"0\", \n",
    "                    \"-myrimatch\", \"0\", \"-andromeda\", \"0\", \"-omssa\", \"0\", \"-tide\", \"0\"],\n",
    "                    check=False, cwd=work_dir, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, universal_newlines=True)\n",
    "    \n",
    "    if search_process.returncode != 0:\n",
    "        print(search_process.stdout)\n",
    "        raise Exception(\"Search process failed.\")\n",
    "    \n",
    "    print(\"Search completed.\")\n",
    "    \n",
    "    # -------------------------------------------------\n",
    "    # Run PeptideShaker\n",
    "    print(\"Processing result using PeptideShaker...\")\n",
    "    peptide_shaker_result_file = os.path.join(work_dir, \"experiment.cpsx\")\n",
    "\n",
    "    peptide_shaker_process = subprocess.run([\"java\", \"-cp\", peptide_shaker_jar,\n",
    "                    \"eu.isas.peptideshaker.cmd.PeptideShakerCLI\",\n",
    "                    \"-experiment\", \"experiment1\",\n",
    "                    \"-sample\", \"test\",\n",
    "                    \"-replicate\", \"1\",\n",
    "                    \"-identification_files\", work_dir,\n",
    "                    \"-out\", peptide_shaker_result_file,\n",
    "                    \"-id_params\", param_file,\n",
    "                    \"-spectrum_files\", spectrum_files],\n",
    "                    check=False, cwd=work_dir, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, universal_newlines=True)\n",
    "    \n",
    "    if peptide_shaker_process.returncode != 0:\n",
    "        print(peptide_shaker_process.stdout)\n",
    "        raise Exception(\"Failed to run PeptideShaker\")\n",
    "\n",
    "    if not os.path.isfile(peptide_shaker_result_file):\n",
    "        raise Exception(\"Failed to process result file.\")\n",
    "      \n",
    "    # ---------------------------------------------------\n",
    "    # create TSV output files\n",
    "    print(\"Converting result to TSV format...\")\n",
    "    conversion_process = subprocess.run([\"java\", \"-cp\", peptide_shaker_jar,\n",
    "                  \"eu.isas.peptideshaker.cmd.ReportCLI\",\n",
    "                  \"-in\", peptide_shaker_result_file,\n",
    "                  \"-out_reports\", work_dir,\n",
    "                  \"-reports\", \"8\"],\n",
    "                  check=False, cwd=work_dir, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, universal_newlines=True)\n",
    "    \n",
    "    if conversion_process.returncode != 0:\n",
    "        print(conversion_process.stdout)\n",
    "        raise Exception(\"Conversion process failed\")\n",
    "    \n",
    "    result_file=os.path.join(work_dir, \"experiment1_test_1_Extended_PSM_Report.txt\")\n",
    "    \n",
    "    if not os.path.isfile(result_file):\n",
    "        raise Exception(\"Error: Conversion failed\")\n",
    "        \n",
    "    print(\"Done.\")\n",
    "    \n",
    "    # -----------------------------------------------------\n",
    "    # Create the experimental design\n",
    "    labelling_method = list(searchUI.labelling.options.keys())[searchUI.labelling.index]\n",
    "    \n",
    "    if labelling_method[0:6] == \"iTRAQ8\":\n",
    "        labelling_method = \"iTRAQ8\"\n",
    "        \n",
    "    expDesignUI = ExpDesignUI(labelling_method)\n",
    "    expDesignUI.display()\n",
    "    \n",
    "    \n",
    "def observe_work_dir_select(change):\n",
    "    global work_dir_select\n",
    "    if change['type'] == \"change\" and change['name'] == \"value\":\n",
    "        # update the required UI controls\n",
    "        searchUI.updateFastaFiles(change[\"new\"])\n",
    "\n",
    "# -------------------\n",
    "# Code to create the UI\n",
    "# --------------------\n",
    "result_file=None\n",
    "work_dir=None\n",
    "peaklist_dir=os.path.abspath(\"IN\")\n",
    "\n",
    "searchUI = SearchUI()\n",
    "searchUI.display()\n",
    "\n",
    "# create parameter list as input for R script\n",
    "Rinput = [searchUI.labelling.value, searchUI.spectra_dir.value]\n",
    "\n",
    "#TODO set names of samples and replicates (peptideshaker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "%%R -i Rinput\n",
    "\n",
    "# reading parameters\n",
    "labeling <- Rinput[[1]]\n",
    "inputDir <- Rinput[[2]]\n",
    "\n",
    "# library causes the execution to fail if the library is missing\n",
    "suppressWarnings(suppressMessages(library(lattice)))\n",
    "suppressWarnings(suppressMessages(library(stringr)))\n",
    "suppressWarnings(suppressMessages(library(mzID)))\n",
    "#suppressWarnings(suppressMessages(library(matrixStats)))\n",
    "#suppressWarnings(suppressMessages(library(venneuler)))\n",
    "suppressWarnings(suppressMessages(library(MSnbase)))\n",
    "\n",
    "# IMPORTANT\n",
    "# remove quotes from mgf-files before conversion:\n",
    "#for file in *mgf; do sed 's/\\\"//g' $file > ${file%.*}_noquote.mgf; done\n",
    "#for file in *_noquote.mgf; do /usr/local/tpp/bin/msconvert \"$file\" \"${file%.*}.mzML\"; done \n",
    "# Add option to provide mzML directly\n",
    "\n",
    "print(inputDir)\n",
    "setwd(inputDir)\n",
    "\n",
    "## Folder names for different runs (e.g. different replicates, ...)\n",
    "samples <- s <- c(\".\")\n",
    "ProtDat <- list()\n",
    "for (s in samples) {\n",
    "  \n",
    "  ## Organize technical runs in the same folder\n",
    "  # filenames\n",
    "  ident_files <- list.files(paste(\"/home/biodocker/OUT/\",s,sep=\"\"),pattern=\"Extended_PSM_Report.txt$\",full.names=T)\n",
    "  #mzML_files <- list.files(s,pattern=\".mzML$\",full.names = T)\n",
    "  #mgf_files <- list.files(s,pattern=\"_noquote.mgf$\",full.names = T)\n",
    "  mgf_files <- list.files(s,pattern=\"mgf$\",full.names = T)\n",
    "  \n",
    "  # TODO load parameters \"%%R -i parname\"\n",
    "  # process the input files\n",
    "  max.fdr <- 0.01\n",
    "  \n",
    "  # translate into labeling method descriptors\n",
    "  quant.methods <- cbind(c(\"TMT10-plex of peptide N-term\",\"TMT 6-plex of K,TMT 6-plex of peptide N-term\",\n",
    "                           \"iTRAQ 4-plex of K,iTRAQ 4-plex of Y,iTRAQ 4-plex of peptide N-term\",\n",
    "                           \"iTRAQ 8-plex of K, iTRAQ 8-plex of peptide N-term\",\"iTRAQ 8-plex of Y\"),\n",
    "                         c(TMT10,TMT6,iTRAQ4,iTRAQ8,iTRAQ8))\n",
    "  quant.method <- quant.methods[quant.methods[,1] == labeling,2]\n",
    "  ## TODO read from experimental design\n",
    "  class.labels <- c(\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \"J\")\n",
    "  args <- commandArgs(trailingOnly = TRUE)\n",
    "  \n",
    "  if (!file.exists(ident_files)) {\n",
    "    stop(\"Error: Cannot find identification file \", ident_filess)\n",
    "  }\n",
    "  for (mgf_file in mgf_files) {\n",
    "    if (!file.exists(mgf_file)) {\n",
    "      stop(\"Error: Cannot find mgf file \", mgf_file)\n",
    "    }\n",
    "  }\n",
    "  \n",
    "  # Convert SearchGUI output to isobar format\n",
    "  psms <- read.csv(ident_files, sep = \"\\t\",stringsAsFactors = F)\n",
    "  \n",
    "  if (! \"Decoy\" %in% names(psms)) {\n",
    "    stop(\"Error: No decoy information available in output file\")\n",
    "  }\n",
    "  \n",
    "  print(paste(\"Loaded\",nrow(psms), \"PSMs\"))\n",
    "  \n",
    "  # ---- Confidence filter ----\n",
    "  psms <- psms[order(psms[, \"Confidence....\"], decreasing = T), ]\n",
    "  decoy.psms <- which(psms[, \"Decoy\"] == \"1\")\n",
    "  \n",
    "  decoy.count <- 0\n",
    "  \n",
    "  for (decoy.index in decoy.psms) {\n",
    "    decoy.count <- decoy.count + 1\n",
    "    target.count <- decoy.index - decoy.count\n",
    "    \n",
    "    cur.fdr <- (decoy.count * 2) / (decoy.count + target.count)\n",
    "    \n",
    "    if (cur.fdr > max.fdr) {\n",
    "      # filter\n",
    "      psms <- psms[1:decoy.index - 1,]\n",
    "      break\n",
    "    }\n",
    "  }\n",
    "  \n",
    "print(paste0(\"Filtered \", nrow(psms), \" PSMs @ \", max.fdr, \" FDR\"))\n",
    "    \n",
    "    print(head(psms))\n",
    "    \n",
    "  #### prepare for MSnbase\n",
    "  psms$rank <- 1\n",
    "  psms$desc <- psms$Protein.s.\n",
    "  psms$spectrumID <- psms$Spectrum.Title #str_extract(psms$Spectrum.Title, \"scan=[0-9]*\")\n",
    "  psms$spectrumFile <- psms$Spectrum.File\n",
    "  psms$idFile <- ident_files\n",
    "  # remove unnecessary PTMs from modified sequence\n",
    "  # TODO: define whether to take oxidation, ...\n",
    "  psms$Modified.Sequence <- gsub(\"<cmm>\",\"\",psms$Modified.Sequence)\n",
    "  psms$Modified.Sequence <- sub(\"[a-z,A-Z]*-\",\"\",psms$Modified.Sequence)\n",
    "  psms$Modified.Sequence <- sub(\"-[a-z,A-Z]*\",\"\",psms$Modified.Sequence)\n",
    "  psms$Modified.Sequence <- gsub(\"<iTRAQ>\",\"\",psms$Modified.Sequence)\n",
    "  psms$Modified.Sequence <- gsub(\"<TMT>\",\"\",psms$Modified.Sequence)\n",
    "  psms$sequence <- psms$Modified.Sequence\n",
    "    #Optional:\n",
    "  # psms$Modified.Sequence <- gsub(\"<ox>\",\"\",psms$Modified.Sequence)\n",
    "  \n",
    "  \n",
    "  \n",
    "  # reading MS and identification data\n",
    "  allSpectra <- list()\n",
    "  for (mgf_file in mgf_files) {\n",
    "    # myExp1 <- readMSData(mzML_files, mode=\"onDisk\", verbose=T)\n",
    "    \n",
    "    # trim mgf file by removing all masses below 130\n",
    "    system(paste(\"awk '$1<130 || !($1 ~ /^[0-9,\\\\.]+/)'\", mgf_file, \"> t.mgf\"))\n",
    "    \n",
    "    \n",
    "    myExp1 <- readMgfData(\"t.mgf\", verbose=T)\n",
    "    # myExp1 <- MSnbase::readMgfData(\"t.mgf\", verbose=T)\n",
    "    system(\"rm t.mgf\")\n",
    "    for (i in 1:ncol(fData(myExp1))) {\n",
    "      if (is.factor(fData(myExp1)[,i]))\n",
    "        fData(myExp1)[,i] <- as.character(fData(myExp1)[,i])\n",
    "    }\n",
    "    fData(myExp1)$spectrumFile <- mgf_file\n",
    "    allSpectra[[mgf_file]] <- myExp1 #updateFeatureNames(myExp1,label = paste(\"Sample\",i))\n",
    "  }\n",
    "\n",
    "  \n",
    "  \n",
    "  # myExp1 <- addIdentificationData(myExp1,psms,decoy=\"Decoy\",rank=\"rank\",acc=\"Protein.s.\",\n",
    "  #                                 icol=\"spectrumID\",fcol=\"spectrumId\",desc=\"desc\",pepseq=\"Modified.Sequence\",verbose=T)\n",
    "  cl <- makeCluster(4)\n",
    "  myExp <- qnt <- list()\n",
    "  for (mgf_file in mgf_files) {\n",
    "    myExp[[mgf_file]] <- addIdentificationData(allSpectra[[mgf_file]],psms,decoy=\"Decoy\",rank=\"rank\",acc=\"Protein.s.\",\n",
    "                                  icol=\"Spectrum.Title\",fcol=\"TITLE\",desc=\"desc\",pepseq=\"Modified.Sequence\",verbose=T)\n",
    "    print(idSummary(myExp[[mgf_file]]))\n",
    "    myExp[[mgf_file]] <- removeNoId(myExp[[mgf_file]])\n",
    "    qnt[[mgf_file]] <- quantify(myExp[[mgf_file]], method=\"max\", reporters = quant.method, strict =F, verbose=T )\n",
    "  } \n",
    "\n",
    "  ## Convert SearchGUI output to file format that can be processed by isobar\n",
    "  stopCluster(cl)\n",
    "  \n",
    "  imp<-makeImpuritiesMatrix(length(quant.method[[1]]),edit=F)\n",
    "  for (i in 1:length(qnt)) {\n",
    "    qnt[[i]] <- purityCorrect(qnt[[i]],imp)\n",
    "    exprs(qnt[[i]]) <- log2(exprs(qnt[[i]]))\n",
    "    qnt[[i]] <- normalise(qnt[[i]],\"center.median\")\n",
    "    qnt[[i]] <- updateFeatureNames(qnt[[i]],label = paste(\"Sample\",i))\n",
    "  }\n",
    "  \n",
    "  names(qnt) <- NULL\n",
    "  allqnt <- do.call(\"combine\",args=qnt)\n",
    "  \n",
    "  allqnt <- filterNA(allqnt, pNA=0.5 )\n",
    "  \n",
    "  ## Here we could start using different methods\n",
    "  allProts <- combineFeatures(allqnt, groupBy=fData(allqnt)$Protein.s., fun=\"medpolish\",na.rm=T)\n",
    "  \n",
    "  boxplot(exprs(allProts))\n",
    "  \n",
    "  pairs(exprs(allProts))\n",
    "  #save\n",
    "  write.exprs(allProts,file=paste(sample,\"/ProtData.csv\",sep=\"\"),sep=\",\")\n",
    "  \n",
    "  ProtDat[[s]] <- allProts <- exprs(allProts)\n",
    "  pca <- princomp(allProts[complete.cases((allProts)),])\n",
    "  plot(pca)\n",
    "  plot(pca$loadings)\n",
    "  text(pca$loadings,colnames(ProtDat))\n",
    "  \n",
    "}"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Hide code",
  "hide_code_all_hidden": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "notify_time": "5"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
