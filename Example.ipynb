{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "# Protocol for analysis of labeled proteomics data\n",
    "We recommend the following structure for describing the protocol and its example workflow. Below you can find an overview of required features of the protocol and its description and rules for best practices.\n",
    "\n",
    "This is the link to the github repository of the protocol \n",
    "TODO add version!\n",
    "https://github.com/ProtProtocols/biocontainer-jupyter\n",
    "\n",
    "Link to docker image:\n",
    "\n",
    "\n",
    "\n",
    "## Abstract\n",
    "\n",
    "Labelled peptide mass spectrometry provides fast large-scale comparison of protein abundances over multiple conditions. To date, no one stop shop software solution exists that enables the common researcher to carry out the full analysis of the acquired raw data. Pipeline for this analysis have often been established in laboratories that are based on a combination of different software tools and in-house programs. Different and often new versions of the used tools and issues with the compatibility of apparently interoperable tools make it very difficult to ensure reproducible proteomics data analysis. We present a 100% reproducible software protocol to fully analyze data from one of the most popular types of proteomics experiments. The protocol is fully based on open source tools installed on a docker container, additionally providing a user-friendly and interactive browser interface for guidance of configuration and execution of the different operations. An example use case is provided that can be used for testing and adaption of own data sets. With this setup, analysis of labelled MS data will yield identical results on any computer that meets the computational bandwidth to run the analysis. \n",
    "\n",
    "(Provide a short description of the software protocol including broader context, functionality, use case and purpose.)\n",
    "\n",
    "\n",
    "## Maintainer\n",
    "Provide details about the protocol maintainer (e.g. email address and/or github username)\n",
    "\n",
    "## Software\n",
    "Specify links for documentation and tutorials of used software, source code, publications and use cases. Detail versions of each used software. Alternatively, provide links to the software descriptions in https://bio.tools where this information is available.\n",
    "\n",
    "## Diagram\n",
    "Provide a simple diagram of functionality of the workflow/software. We recommend using controlled vocabularies for input/output data types and file formats as well as provided operation of the tool(s). You can use http://edamontology.org terms for the description.\n",
    "\n",
    "__TODO: example__\n",
    "\n",
    "## System requirements\n",
    "Fill in the following items:\n",
    "Required hard disk space for docker image, input and output files: \n",
    "\n",
    "Required memory: \n",
    "\n",
    "Recommmended number of threads: \n",
    "\n",
    "## Example \n",
    "Presentation of well-documented instructions and commands to run the example use case. Depending on the use case and the software, provide link(s) to open the web service incorportated in the Docker image (e.g. 0.0.0.0:8080), bash commands to run programs from the command line and additional code for e.g. checking and visualizing the (intermediate) results. \n",
    "\n",
    "Instead of providing the instructions in this notebook, one can also provide a link to a notebook containing the example use case.\n",
    "\n",
    "## More general use case (optional)\n",
    "Provide link to notebook with a generalized use case that easily can be adapted to e.g. process different input data and concurrent parametrization.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "# Example\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO list\n",
    "\n",
    "- Add possibility of samples that are arranged in different folders\n",
    "- Evaluate TMTc: https://pubs.acs.org/doi/10.1021/acs.analchem.7b04713\n",
    "- Protein inference function (can be iPQF)\n",
    "- Protein summarization function\n",
    "- Separate R scripts for quantification of psms, protein level stats and rest\n",
    "- Run scripts interactively through buttons\n",
    "- Save all configurations in file\n",
    "- Remove all temporary files before running search\n",
    "- Check for available memory and adapt peptideshaker settings\n",
    "\n",
    "**Completed:**\n",
    "- Python script: Fix search with new PeptideShaker version\n",
    "- Python script: Adapt all MGF titles prior to search\n",
    "- Remove quotes from titles in mgf-files\n",
    "- Python script: Add MGF Peak filter function (100 - 150)\n",
    "- Changed to iTRAQ4/8 having Y labeled either a variable or fixed PTM (inspired by Mascot settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-10T07:31:35.045013Z",
     "start_time": "2018-04-10T07:31:33.901493Z"
    },
    "hideCode": true,
    "hideOutput": true,
    "hidePrompt": false,
    "hide_input": true,
    "init_cell": true,
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "%load_ext rpy2.ipython"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "Specify parameters for database search and evaluation of identified peptide-spectrum matches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-10T08:27:07.359646Z",
     "start_time": "2018-04-10T08:27:03.937941Z"
    },
    "code_folding": [
     7,
     169
    ],
    "hideCode": false,
    "hidePrompt": false,
    "hide_input": true,
    "init_cell": true,
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe8468a8e983456cb021db561c2d156e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>VBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "VBox(children=(Label(value='Working directory'), Dropdown(options={'Example files': '/home/biodocker/', '/data/': '/data/'}, value='/home/biodocker/'), Label(value='Precursor tolerance (ppm):'), IntSlider(value=20, max=30, min=-10), Label(value='Fragment ion tolerance (da):'), BoundedFloatText(value=0.05, max=200.0), Label(value='Fasta file (database, must NOT contain decoy sequences):'), Dropdown(options={'IN/sp_human_concatenated_target_decoy_concatenated_target_decoy.fasta': '/home/biodocker/IN/sp_human_concatenated_target_decoy_concatenated_target_decoy.fasta', 'IN/sp_human_concatenated_target_decoy_concatenated_target_decoy_concatenated_target_decoy.fasta': '/home/biodocker/IN/sp_human_concatenated_target_decoy_concatenated_target_decoy_concatenated_target_decoy.fasta', 'IN/sp_human.fasta': '/home/biodocker/IN/sp_human.fasta', 'IN/sp_human_concatenated_target_decoy.fasta': '/home/biodocker/IN/sp_human_concatenated_target_decoy.fasta'}, value='/home/biodocker/IN/sp_human_concatenated_target_decoy_concatenated_target_decoy.fasta'), Checkbox(value=True, description='Generate decoy sequences'), Label(value='Quantification method:'), Dropdown(options={'TMT10': 'TMT 10-plex of K,TMT 10-plex of peptide N-term', 'iTRAQ8 (Y fixed)': 'iTRAQ 8-plex of K,iTRAQ 8-plex of Y,iTRAQ 8-plex of peptide N-term', 'iTRAQ8 (Y variable)': 'iTRAQ 8-plex of K,iTRAQ 8-plex of peptide N-term', 'iTRAQ4 (Y variable)': 'iTRAQ 4-plex of K,iTRAQ 4-plex of peptide N-term', 'iTRAQ4 (Y fixed)': 'iTRAQ 4-plex of K,iTRAQ 4-plex of Y,iTRAQ 4-plex of peptide N-term', 'TMT6': 'TMT 6-plex of K,TMT 6-plex of peptide N-term'}, value='TMT 10-plex of K,TMT 10-plex of peptide N-term'), Label(value='Number of miscleavages;'), IntSlider(value=1, max=10), Label(value='Further fixed modifications'), Dropdown(options=('Carbamidomethylation of C', 'None'), value='Carbamidomethylation of C'), Label(value='Further variable modifications (Hold Ctrl to select multiple)'), SelectMultiple(index=(0,), options=('Oxidation of M', 'Phosphorylation of STY', 'Acetylation of peptide N-term', 'Acetylation of protein N-term'), value=('Oxidation of M',)), Label(value='Folder for spectra files (files need to be mgf)'), Dropdown(options={'IN': '/home/biodocker/IN', 'bin': '/home/biodocker/bin', 'LOG': '/home/biodocker/LOG', 'misc': '/home/biodocker/misc', 'OUT': '/home/biodocker/OUT'}, value='/home/biodocker/IN'), Button(description='Run Search', icon='check', style=ButtonStyle(), tooltip='Launch the search')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adapting MGF titles...\n",
      "Extracting reporter peaks...\n",
      "Creating decoy database...\n"
     ]
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from ipywidgets import VBox, Label\n",
    "import sys, os\n",
    "import subprocess\n",
    "\n",
    "# create an empty class as a storage for all UI widgets\n",
    "class SearchUI:\n",
    "    def __init__(self):\n",
    "        self.work_dir_select = widgets.Dropdown(options={'/data/': '/data/', 'Example files': '/home/biodocker/'}, \n",
    "                                                value='/home/biodocker/')\n",
    "        self.work_dir_select.observe(observe_work_dir_select)\n",
    "        \n",
    "        self.precursor_tolerance = widgets.IntSlider(min=-10,max=30,step=1,value=20)\n",
    "        self.fragment_tolerance = widgets.BoundedFloatText(min=0,max=200,value=0.05)\n",
    "        self.fasta_db = widgets.Dropdown(options={\"sp_human.fasta\": \"IN/sp_human.fasta\"})\n",
    "        \n",
    "        self.generate_decoy = widgets.Checkbox(value=True, description=\"Generate decoy sequences\")\n",
    "        \n",
    "        # TODO  needs table to describe labeling formats\n",
    "        self.labelling = widgets.Dropdown(options=\n",
    "                          {'TMT6': 'TMT 6-plex of K,TMT 6-plex of peptide N-term',\n",
    "                           'TMT10': 'TMT 10-plex of K,TMT 10-plex of peptide N-term',\n",
    "                           'iTRAQ4 (Y fixed)': 'iTRAQ 4-plex of K,iTRAQ 4-plex of Y,iTRAQ 4-plex of peptide N-term',\n",
    "                           'iTRAQ4 (Y variable)': 'iTRAQ 4-plex of K,iTRAQ 4-plex of peptide N-term',\n",
    "                           'iTRAQ8 (Y fixed)': 'iTRAQ 8-plex of K,iTRAQ 8-plex of Y,iTRAQ 8-plex of peptide N-term',\n",
    "                           'iTRAQ8 (Y variable)': 'iTRAQ 8-plex of K,iTRAQ 8-plex of peptide N-term'},\n",
    "                      value='TMT 10-plex of K,TMT 10-plex of peptide N-term')\n",
    "        \n",
    "        self.missed_cleavages = widgets.IntSlider(min=0,max=10,step=1,value=1)\n",
    "        self.fixed_ptms = widgets.Dropdown(options=[\"Carbamidomethylation of C\",\"None\"])\n",
    "\n",
    "        # PTMs\n",
    "        self.var_ptms = widgets.SelectMultiple(\n",
    "            options=[\"Oxidation of M\",\n",
    "                     \"Phosphorylation of STY\",\n",
    "                     \"Acetylation of peptide N-term\",\n",
    "                     \"Acetylation of protein N-term\"],\n",
    "            value=['Oxidation of M'])\n",
    "        \n",
    "        self.spectra_dir = widgets.Dropdown(options={\"IN\": \"IN\"})\n",
    "\n",
    "        # ww = widgets.Checkbox(description=\"Decoy\")\n",
    "\n",
    "        self.search_button = widgets.Button(\n",
    "            description='Run Search',\n",
    "            disabled=False,\n",
    "            button_style='', # 'success', 'info', 'warning', 'danger' or ''\n",
    "            tooltip='Launch the search',\n",
    "            icon='check'\n",
    "        )\n",
    "\n",
    "        self.search_button.on_click(run_search)\n",
    "        \n",
    "        \n",
    "    def updateFastaFiles(self, workdir):\n",
    "        # get all FASTA files\n",
    "        fasta_files = [file for file in os.listdir(workdir) if file[-6:] == \".fasta\"]\n",
    "        \n",
    "        # also search all subdirectories for FASTA files\n",
    "        for d in os.listdir(workdir):\n",
    "            d_path = os.path.join(workdir, d)\n",
    "            if os.path.isdir(d_path) and d[0] != \".\":\n",
    "                fasta_files += [os.path.join(d, file) for file in os.listdir(d_path) if file[-6:] == \".fasta\"]\n",
    "        \n",
    "        # create the dict to add as values to the control\n",
    "        file_list = dict()\n",
    "        sel_value = None\n",
    "        \n",
    "        for f in fasta_files:\n",
    "            file_list[f] = os.path.join(os.path.abspath(workdir), f)\n",
    "            if sel_value is None:\n",
    "                sel_value = os.path.join(os.path.abspath(workdir), f)\n",
    "        \n",
    "        self.fasta_db.options = file_list\n",
    "        self.fasta_db.value = sel_value\n",
    "        \n",
    "        # update the list of possible peaklist directories\n",
    "        directories = [d for d in os.listdir(workdir) if os.path.isdir(os.path.join(workdir, d)) and d[0] != \".\"]\n",
    "        \n",
    "        dir_list = dict()\n",
    "        \n",
    "        for d in directories:\n",
    "            dir_list[d] = os.path.join(os.path.abspath(workdir), d)\n",
    "            \n",
    "        self.spectra_dir.options = dir_list\n",
    "        \n",
    "        if \"IN\" in dir_list:\n",
    "            self.spectra_dir.value = dir_list[\"IN\"]\n",
    "        \n",
    "        \n",
    "    def display(self):\n",
    "        self.updateFastaFiles(self.work_dir_select.value)\n",
    "        \n",
    "        settings_box = VBox([Label('Working directory'), self.work_dir_select,\n",
    "                             Label('Precursor tolerance (ppm):'), self.precursor_tolerance, \n",
    "                             Label('Fragment ion tolerance (da):'), self.fragment_tolerance,\n",
    "                             Label('Fasta file (database, must NOT contain decoy sequences):'), self.fasta_db,\n",
    "                             self.generate_decoy,\n",
    "                             Label('Quantification method:'), self.labelling,\n",
    "                             Label('Number of miscleavages;'), self.missed_cleavages,\n",
    "                             Label('Further fixed modifications'), self.fixed_ptms,\n",
    "                             Label('Further variable modifications (Hold Ctrl to select multiple)'), self.var_ptms,\n",
    "                             Label('Folder for spectra files (files need to be mgf)'), self.spectra_dir,\n",
    "                             self.search_button])\n",
    "\n",
    "        display(settings_box)\n",
    "        \n",
    "\n",
    "class ExpDesignUI:\n",
    "    def __init__(self, labelling_technique):\n",
    "        \"\"\"\n",
    "        Generates all use interface objects as member variables.\n",
    "        \n",
    "        :param labelling_technique: The labelling method used.\n",
    "        \"\"\"\n",
    "        # always expect two groups\n",
    "        self.group1_name = widgets.Text(placeholder = \"Treatment\", description = \"Group 1:\")\n",
    "        self.group2_name = widgets.Text(placeholder = \"Control\", description = \"Group 2:\")\n",
    "        \n",
    "        self.channels = {\n",
    "            'TMT6': [\"126\", \"127\", \"128\", \"129\", \"130\", \"131\"],\n",
    "            'TMT10': [\"126\", \"127N\", \"127C\", \"128N\", \"128C\", \"129N\", \"129C\", \"130N\", \"130C\", \"131\"],\n",
    "            'iTRAQ4': [\"114\", \"115\", \"116\", \"117\"],\n",
    "            'iTRAQ8': [\"113\", \"114\", \"115\", \"116\", \"117\", \"118\", \"119\", \"121\"]\n",
    "        }\n",
    "        \n",
    "        # removed everything in string labellign_technique after space \n",
    "        if labelling_technique.split(\" \")[0] not in self.channels:\n",
    "            raise Exception(\"Unknown labelling technique: '\" + labelling_technique + \"'\")\n",
    "            \n",
    "        self.labelling_technique = labelling_technique.split(\" \")[0]\n",
    "            \n",
    "        # generate the textfields for the channels\n",
    "        self.channel_names = list()\n",
    "        \n",
    "        for channel in self.channels[self.labelling_technique]:\n",
    "            self.channel_names.append(widgets.Text(description = channel, placeholder = \"Sample \" + channel))\n",
    "            \n",
    "        # add select boxes to select the experimental group\n",
    "        self.group_selects = list()\n",
    "        \n",
    "        for channel in self.channels[self.labelling_technique]:\n",
    "            self.group_selects.append(widgets.Dropdown(options = [\"Group 1\", \"Group 2\"], value = \"Group 1\"))\n",
    "            \n",
    "        self.save_button = widgets.Button(\n",
    "            description='Save design',\n",
    "            disabled=False,\n",
    "            button_style='', # 'success', 'info', 'warning', 'danger' or ''\n",
    "            tooltip='Save the experimental design',\n",
    "            icon='check'\n",
    "        )\n",
    "\n",
    "        self.save_button.on_click(self.save_design)\n",
    "\n",
    "    def display(self):\n",
    "        widget_list = [widgets.Label(\"Treatment group names:\"), self.group1_name, self.group2_name,\n",
    "                       widgets.Label(\"Sample names (per channel):\")]\n",
    "        \n",
    "        for i in range(0, len(self.channel_names)):\n",
    "            widget_list.append(widgets.HBox([self.channel_names[i], self.group_selects[i]]))\n",
    "            \n",
    "        widget_list.append(self.save_button)\n",
    "        \n",
    "        widget_box = VBox(widget_list)\n",
    "        \n",
    "        display(widget_box)\n",
    "        \n",
    "    def save_design(self):\n",
    "        pass\n",
    "      \n",
    "        \n",
    "def adapt_mgf_titles(filenames):\n",
    "    \"\"\"\n",
    "    This function changes all MGF titles to [filename].[spec index]\n",
    "    :param: filenames: Filenames of the MGF files to change\n",
    "    \"\"\"\n",
    "    for filename in filenames:\n",
    "        with open(filename, \"r\") as reader:\n",
    "            clean_name = os.path.basename(filename).replace(\" \", \"_\")\n",
    "            # MGF index reference in PSI standard is 1-based\n",
    "            cur_index = 1\n",
    "            \n",
    "            with open(filename + \".tmp\", \"w\") as writer:\n",
    "                for line in reader:\n",
    "                    if line[0:6] == \"TITLE=\":\n",
    "                        writer.write(\"TITLE=\" + clean_name + \".\" + str(cur_index) + \"\\n\")\n",
    "                        cur_index += 1\n",
    "                    else:\n",
    "                        writer.write(line)\n",
    "                        \n",
    "        # backup the original file\n",
    "        os.rename(filename, filename + \".org\")\n",
    "        os.rename(filename + \".tmp\", filename)\n",
    "        \n",
    "        \n",
    "def filter_mgf_peaks(filenames, min_mz=100, max_mz=150):\n",
    "    \"\"\"\n",
    "    Removes all peaks from the passed mgf files that are below min_mz or\n",
    "    above max_mz. The results are written to files with the same name but\n",
    "    \".filtered\" appended to the name.\n",
    "    :param: filenames: List of filenames to process.\n",
    "    :param: min_mz: Minimum m/z a peak must have to be kept\n",
    "    :param: max_mz: Maximum m/z a peak may have to be kept\n",
    "    \"\"\"\n",
    "    for filename in filenames:\n",
    "        with open(filename, \"r\") as reader:\n",
    "            with open(filename + \".filtered\", \"w\") as writer:\n",
    "                for line in reader:\n",
    "                    # check if it's a peak\n",
    "                    if line[0].isdigit():\n",
    "                        sep_index = line.find(\" \")\n",
    "                        if sep_index < 0:\n",
    "                            sep_index = line.find(\"\\t\")\n",
    "                        if sep_index < 0:\n",
    "                            raise Exception(\"Invalid peak definition found: \" + line + \n",
    "                                            \". Failed to filter file \" + filename)\n",
    "                            \n",
    "                        mz = float(line[:sep_index])\n",
    "                            \n",
    "                        # ignore any non-matching peaks\n",
    "                        if mz < min_mz or mz > max_mz:\n",
    "                            continue\n",
    "                            \n",
    "                    # copy the line\n",
    "                    writer.write(line)\n",
    "    \n",
    "        \n",
    "def run_search(button):\n",
    "    global searchUI, result_file\n",
    "    \n",
    "    # make sure all required fields were selected\n",
    "    if searchUI.work_dir_select.value is None:\n",
    "        print(\"Error: No working directory selected\")\n",
    "        return\n",
    "    \n",
    "    if searchUI.fasta_db.value is None:\n",
    "        print(\"Error: No FASTA file selected\")\n",
    "        return\n",
    "    \n",
    "    # create the directory paths to work in\n",
    "    peaklist_dir = os.path.abspath(searchUI.spectra_dir.value)\n",
    "    \n",
    "    if not os.path.isdir(peaklist_dir):\n",
    "        raise Exception(\"Invalid peak list directory selected: \" + peaklist_dir + \" does not exist.\")\n",
    "    \n",
    "    peptide_shaker_jar = \"/home/biodocker/bin/PeptideShaker-1.16.17/PeptideShaker-1.16.17.jar\"\n",
    "    searchgui_jar = \"/home/biodocker/bin/SearchGUI-3.2.20/SearchGUI-3.2.20.jar\"\n",
    "    work_dir = os.path.abspath(os.path.join(searchUI.work_dir_select.value, \"OUT\"))\n",
    "    \n",
    "    # the searches should be performed in the \"OUT\" directory\n",
    "    if not os.path.isdir(work_dir):\n",
    "        os.mkdir(work_dir)\n",
    "        \n",
    "    \n",
    "    # -------------------------------------\n",
    "    # Fix all MGF titles\n",
    "    print(\"Adapting MGF titles...\")\n",
    "    mgf_filenames = [os.path.join(peaklist_dir, f) for f in os.listdir(peaklist_dir) if f[-4:].lower() == \".mgf\"]\n",
    "    adapt_mgf_titles(mgf_filenames)\n",
    "    \n",
    "    print(\"Extracting reporter peaks...\")\n",
    "    filter_mgf_peaks(mgf_filenames)\n",
    "    \n",
    "        \n",
    "    # -------------------------------------\n",
    "    # Generate the decoy database\n",
    "    \n",
    "    if searchUI.generate_decoy.value == True:\n",
    "        print(\"Creating decoy database...\")\n",
    "\n",
    "        # create the decoy database\n",
    "        subprocess.run([\"java\", \"-cp\", searchgui_jar, \n",
    "                       \"eu.isas.searchgui.cmd.FastaCLI\", \"-in\", searchUI.fasta_db.value, \"-decoy\"], check=True,\n",
    "                       cwd=work_dir,\n",
    "                       stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "\n",
    "        # get the filename of the decoy database\n",
    "        database_file = os.path.abspath(searchUI.fasta_db.value)[:-6] + \"_concatenated_target_decoy.fasta\"\n",
    "    else:\n",
    "        # simply use the selected database file\n",
    "        database_file = os.path.abspath(searchUI.fasta_db.value)\n",
    "    \n",
    "    if not os.path.isfile(database_file):\n",
    "        raise Exception(\"Failed to find generated decoy database\")\n",
    "        \n",
    "    # ---------------------------------------------\n",
    "    # Create the search parameter file\n",
    "        \n",
    "    # build the arguments to create the parameter file\n",
    "    param_file = os.path.join(work_dir, \"search.par\")\n",
    "    \n",
    "    # remove any old parameters\n",
    "    if os.path.isfile(param_file):\n",
    "        os.remove(param_file)\n",
    "    \n",
    "    search_args = [\"java\", \"-cp\", searchgui_jar,\n",
    "                   \"eu.isas.searchgui.cmd.IdentificationParametersCLI\",\n",
    "                   \"-out\", param_file]\n",
    "    \n",
    "    # precursor tolerance\n",
    "    search_args.append(\"-prec_tol\")\n",
    "    search_args.append(str(searchUI.precursor_tolerance.value))\n",
    "    # fragment tolerance\n",
    "    search_args.append(\"-frag_tol\")\n",
    "    search_args.append(str(searchUI.fragment_tolerance.value))\n",
    "    # fixed mods\n",
    "    # TODO: labelling cannot always be set as fixed mod???\n",
    "    fixed_mod_string = str(searchUI.labelling.value) + \",\" + str(searchUI.fixed_ptms.value)\n",
    "    search_args.append(\"-fixed_mods\")\n",
    "    search_args.append(fixed_mod_string)\n",
    "    # database\n",
    "    search_args.append(\"-db\")\n",
    "    search_args.append(database_file)\n",
    "    # missed cleavages\n",
    "    search_args.append(\"-mc\")\n",
    "    search_args.append(str(searchUI.missed_cleavages.value))\n",
    "    \n",
    "    # var mods\n",
    "    labelling_method = list(searchUI.labelling.options.keys())[searchUI.labelling.index]\n",
    "    print(labelling_method)\n",
    "    \n",
    "    if len(searchUI.var_ptms.value) > 0 or (\"Y variable\" in labelling_method) :\n",
    "        search_args.append(\"-variable_mods\")\n",
    "        var_mod_list = list()\n",
    "        \n",
    "        for var_mod in searchUI.var_ptms.value:\n",
    "            if var_mod == \"Phosphorylation of STY\":\n",
    "                var_mod_list += [\"Phosphorylation of S\", \"Phosphorylation of T\", \"Phosphorylation of Y\"]\n",
    "            else:\n",
    "                var_mod_list.append(var_mod)\n",
    "        if (labelling_method == \"iTRAQ4 (Y variable)\"):\n",
    "            var_mod_list.append(\"iTRAQ 4-plex of Y\")\n",
    "        if (labelling_method == \"iTRAQ8 (Y variable)\"):\n",
    "            var_mod_list.append(\"iTRAQ 8-plex of Y\")\n",
    "        \n",
    "                \n",
    "        search_args.append(\",\".join(var_mod_list))\n",
    "        \n",
    "    # create the search parameter file\n",
    "    print(\"Creating search parameter file...\")\n",
    "    # print(\" \".join(search_args))\n",
    "    subprocess.run(search_args, check=True, cwd=work_dir, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    \n",
    "    if not os.path.isfile(param_file):\n",
    "        raise Exception(\"Failed to create search parameters\")\n",
    "        \n",
    "    # ------------------------------------------------\n",
    "    # Run the search\n",
    "    print(\"Running search...\")\n",
    "    # TODO: create list of spectrum files - or the folder\n",
    "    spectrum_files = peaklist_dir\n",
    "    print(\"  Searching files in \" + spectrum_files)\n",
    "    search_process = subprocess.run([\"java\", \"-cp\", searchgui_jar,\n",
    "                    \"eu.isas.searchgui.cmd.SearchCLI\", \"-spectrum_files\", spectrum_files,\n",
    "                    \"-output_folder\", work_dir, \"-id_params\", param_file,\n",
    "                    \"-xtandem\", \"0\", \"-msgf\", \"1\", \"-comet\", \"0\", \"-ms_amanda\", \"0\", \n",
    "                    \"-myrimatch\", \"0\", \"-andromeda\", \"0\", \"-omssa\", \"0\", \"-tide\", \"0\"],\n",
    "                    check=False, cwd=work_dir, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, universal_newlines=True)\n",
    "    \n",
    "    if search_process.returncode != 0:\n",
    "        print(search_process.stdout)\n",
    "        raise Exception(\"Search process failed.\")\n",
    "    \n",
    "    print(\"Search completed.\")\n",
    "    \n",
    "    # -------------------------------------------------\n",
    "    # Run PeptideShaker\n",
    "    print(\"Processing result using PeptideShaker...\")\n",
    "    peptide_shaker_result_file = os.path.join(work_dir, \"experiment.cpsx\")\n",
    "\n",
    "    peptide_shaker_process = subprocess.run([\"java\", \"-cp\", peptide_shaker_jar,\n",
    "                    \"eu.isas.peptideshaker.cmd.PeptideShakerCLI\",\n",
    "                    \"-experiment\", \"experiment1\",\n",
    "                    \"-sample\", \"test\",\n",
    "                    \"-replicate\", \"1\",\n",
    "                    \"-identification_files\", work_dir,\n",
    "                    \"-out\", peptide_shaker_result_file,\n",
    "                    \"-id_params\", param_file,\n",
    "                    \"-spectrum_files\", spectrum_files],\n",
    "                    check=False, cwd=work_dir, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, universal_newlines=True)\n",
    "    \n",
    "    if peptide_shaker_process.returncode != 0:\n",
    "        print(peptide_shaker_process.stdout)\n",
    "        raise Exception(\"Failed to run PeptideShaker\")\n",
    "\n",
    "    if not os.path.isfile(peptide_shaker_result_file):\n",
    "        raise Exception(\"Failed to process result file.\")\n",
    "      \n",
    "    # ---------------------------------------------------\n",
    "    # create TSV output files\n",
    "    print(\"Converting result to TSV format...\")\n",
    "    conversion_process = subprocess.run([\"java\", \"-cp\", peptide_shaker_jar,\n",
    "                  \"eu.isas.peptideshaker.cmd.ReportCLI\",\n",
    "                  \"-in\", peptide_shaker_result_file,\n",
    "                  \"-out_reports\", work_dir,\n",
    "                  \"-reports\", \"8\"],\n",
    "                  check=False, cwd=work_dir, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, universal_newlines=True)\n",
    "    \n",
    "    if conversion_process.returncode != 0:\n",
    "        print(conversion_process.stdout)\n",
    "        raise Exception(\"Conversion process failed\")\n",
    "    \n",
    "    result_file=os.path.join(work_dir, \"experiment1_test_1_Extended_PSM_Report.txt\")\n",
    "    \n",
    "    if not os.path.isfile(result_file):\n",
    "        raise Exception(\"Error: Conversion failed\")\n",
    "        \n",
    "    print(\"Done.\")\n",
    "    \n",
    "    # -----------------------------------------------------\n",
    "    # Create the experimental design\n",
    "           \n",
    "    expDesignUI = ExpDesignUI(labelling_method)\n",
    "    expDesignUI.display()\n",
    "    \n",
    "    \n",
    "def observe_work_dir_select(change):\n",
    "    global work_dir_select\n",
    "    if change['type'] == \"change\" and change['name'] == \"value\":\n",
    "        # update the required UI controls\n",
    "        searchUI.updateFastaFiles(change[\"new\"])\n",
    "\n",
    "# -------------------\n",
    "# Code to create the UI\n",
    "# --------------------\n",
    "result_file=None\n",
    "work_dir=None\n",
    "peaklist_dir=os.path.abspath(\"IN\")\n",
    "\n",
    "searchUI = SearchUI()\n",
    "searchUI.display()\n",
    "\n",
    "# create parameter list as input for R script\n",
    "Rinput = [searchUI.labelling.value, searchUI.spectra_dir.value]\n",
    "\n",
    "#TODO set names of samples and replicates (peptideshaker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-10T07:45:55.348039Z",
     "start_time": "2018-04-10T07:45:55.335567Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "717202834eda43b08f250fab82016dee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>Button</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "Button(description='Run quantification and peptide inference', layout=Layout(width='30%'), style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.execute_cell_range(IPython.notebook.get_selected_index()+1, IPython.notebook.get_selected_index()+2)"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Javascript, display\n",
    "from ipywidgets import widgets, Layout\n",
    "\n",
    "def run_all(ev):\n",
    "    display(Javascript('IPython.notebook.execute_cell_range(IPython.notebook.get_selected_index()+1, IPython.notebook.get_selected_index()+2)'))\n",
    "\n",
    "button = widgets.Button(description=\"Run quantification and peptide inference\",layout=Layout(width='30%'))\n",
    "button.on_click(run_all)\n",
    "display(button)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-06T13:36:52.780952Z",
     "start_time": "2018-04-06T13:36:52.770331Z"
    },
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]]\n",
      "[1] \"TMT10\"\n",
      "\n",
      "[[2]]\n",
      "[1] \"/home/biodocker/IN\"\n",
      "\n",
      "[1] \"/home/biodocker/IN\"\n",
      "Error in get(quant.methods[quant.methods[, 1] == labeling, 2]) : \n",
      "  invalid first argument\n"
     ]
    }
   ],
   "source": [
    "%%R -i Rinput\n",
    "\n",
    "print(Rinput)\n",
    "\n",
    "\n",
    "# reading parameters\n",
    "labeling <- Rinput[[1]]\n",
    "inputDir <- Rinput[[2]]\n",
    "\n",
    "# library causes the execution to fail if the library is missing\n",
    "suppressWarnings(suppressMessages(library(lattice)))\n",
    "suppressWarnings(suppressMessages(library(stringr)))\n",
    "suppressWarnings(suppressMessages(library(mzID)))\n",
    "#suppressWarnings(suppressMessages(library(matrixStats)))\n",
    "#suppressWarnings(suppressMessages(library(venneuler)))\n",
    "suppressWarnings(suppressMessages(library(MSnbase)))\n",
    "\n",
    "# warnings as stdout\n",
    "sink(stdout(), type = \"message\")\n",
    "\n",
    "# IMPORTANT\n",
    "# remove quotes from mgf-files before conversion:\n",
    "#for file in *mgf; do sed 's/\\\"//g' $file > ${file%.*}_noquote.mgf; done\n",
    "#for file in *_noquote.mgf; do /usr/local/tpp/bin/msconvert \"$file\" \"${file%.*}.mzML\"; done \n",
    "# Add option to provide mzML directly\n",
    "\n",
    "print(inputDir)\n",
    "setwd(inputDir)\n",
    "\n",
    "## Folder names for different runs (e.g. different replicates, ...)\n",
    "samples <- s <- c(\".\")\n",
    "ProtDat <- list()\n",
    "for (s in samples) {\n",
    "  \n",
    "  ## Organize technical runs in the same folder\n",
    "  # filenames\n",
    "  ident_files <- list.files(paste(\"/home/biodocker/OUT/\",s,sep=\"\"),pattern=\"Extended_PSM_Report.txt$\",full.names=T)\n",
    "  #mzML_files <- list.files(s,pattern=\".mzML$\",full.names = T)\n",
    "  #mgf_files <- list.files(s,pattern=\"_noquote.mgf$\",full.names = T)\n",
    "  mgf_files <- list.files(s,pattern=\"mgf$\",full.names = T)\n",
    "  \n",
    "  # TODO load parameters \"%%R -i parname\"\n",
    "  # process the input files\n",
    "  max.fdr <- 0.01\n",
    "  \n",
    "  # translate into labeling method descriptors\n",
    "  quant.methods <- cbind(c(\"TMT 10-plex of K,TMT 10-plex of peptide N-term\",\"TMT 6-plex of K,TMT 6-plex of peptide N-term\",\n",
    "                           \"iTRAQ 4-plex of K,iTRAQ 4-plex of Y, iTRAQ 4-plex of peptide N-term\",\n",
    "                           \"iTRAQ 8-plex of K,iTRAQ 8-plex of Y,iTRAQ 8-plex of peptide N-term\",\n",
    "                          \"iTRAQ 4-plex of K,iTRAQ 4-plex of peptide N-term\",\n",
    "                           \"iTRAQ 8-plex of K,iTRAQ 8-plex of peptide N-term\"),\n",
    "                         c(\"TMT10\",\"TMT6\",\"iTRAQ4\",\"iTRAQ8\",\"iTRAQ4\", \"iTRAQ8\"))\n",
    "    if(is.null(quant.methods[quant.methods[,1] == labeling,2])) {\n",
    "        stop(\"Error: labeling method not available\")\n",
    "    }\n",
    "  quant.method <- get(quant.methods[quant.methods[,1] == labeling,2])\n",
    "  \n",
    "    \n",
    "    \n",
    "   ## TODO read from experimental design\n",
    "  class.labels <- c(\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \"J\")\n",
    "  args <- commandArgs(trailingOnly = TRUE)\n",
    "  \n",
    "  if (is.null(ident_files)) {\n",
    "    stop(\"Error: No identification files\")\n",
    "  }\n",
    "\n",
    "      if (is.null(mgf_files)) {\n",
    "    stop(\"Error: No spectrum files\")\n",
    "  }\n",
    "\n",
    "    \n",
    "  if (!file.exists(ident_files)) {\n",
    "    stop(\"Error: Cannot find identification files \", ident_files)\n",
    "  }\n",
    "  for (mgf_file in mgf_files) {\n",
    "    if (!file.exists(mgf_file)) {\n",
    "      stop(\"Error: Cannot find mgf file \", mgf_file)\n",
    "    }\n",
    "  }\n",
    "  \n",
    "  # Convert SearchGUI output to isobar format\n",
    "  psms <- read.csv(ident_files, sep = \"\\t\",stringsAsFactors = F)\n",
    "  \n",
    "  if (! \"Decoy\" %in% names(psms)) {\n",
    "    stop(\"Error: No decoy information available in output file\")\n",
    "  }\n",
    "  \n",
    "  print(paste(\"Loaded\",nrow(psms), \"PSMs\"))\n",
    "  \n",
    "  # ---- Confidence filter ----\n",
    "  psms <- psms[order(psms[, \"Confidence....\"], decreasing = T), ]\n",
    "  decoy.psms <- which(psms[, \"Decoy\"] == \"1\")\n",
    "  \n",
    "  decoy.count <- 0\n",
    "  \n",
    "  for (decoy.index in decoy.psms) {\n",
    "    decoy.count <- decoy.count + 1\n",
    "    target.count <- decoy.index - decoy.count\n",
    "    \n",
    "    cur.fdr <- (decoy.count * 2) / (decoy.count + target.count)\n",
    "    \n",
    "    if (cur.fdr > max.fdr) {\n",
    "      # filter\n",
    "      psms <- psms[1:decoy.index - 1,]\n",
    "      break\n",
    "    }\n",
    "  }\n",
    "  \n",
    "print(paste0(\"Filtered \", nrow(psms), \" PSMs @ \", max.fdr, \" FDR\"))\n",
    "    \n",
    "    #print(head(psms))\n",
    "    \n",
    "  #### prepare for MSnbase\n",
    "  psms$rank <- 1\n",
    "  psms$desc <- psms$Protein.s.\n",
    "  psms$spectrumID <- psms$Spectrum.Title #str_extract(psms$Spectrum.Title, \"scan=[0-9]*\")\n",
    "  psms$spectrumFile <- psms$Spectrum.File\n",
    "  psms$idFile <- ident_files\n",
    "  # remove unnecessary PTMs from modified sequence\n",
    "  # TODO: define whether to take oxidation, ...\n",
    "  psms$Modified.Sequence <- gsub(\"<cmm>\",\"\",psms$Modified.Sequence)\n",
    "  psms$Modified.Sequence <- sub(\"[a-z,A-Z]*-\",\"\",psms$Modified.Sequence)\n",
    "  psms$Modified.Sequence <- sub(\"-[a-z,A-Z]*\",\"\",psms$Modified.Sequence)\n",
    "  psms$Modified.Sequence <- gsub(\"<iTRAQ>\",\"\",psms$Modified.Sequence)\n",
    "  psms$Modified.Sequence <- gsub(\"<TMT>\",\"\",psms$Modified.Sequence)\n",
    "  psms$sequence <- psms$Modified.Sequence\n",
    "    #Optional:\n",
    "  # psms$Modified.Sequence <- gsub(\"<ox>\",\"\",psms$Modified.Sequence)\n",
    "  \n",
    "  \n",
    "  \n",
    "  # reading MS and identification data\n",
    "  allSpectra <- list()\n",
    "  for (mgf_file in mgf_files) {\n",
    "    # myExp1 <- readMSData(mzML_files, mode=\"onDisk\", verbose=T)\n",
    "    \n",
    "    myExp1 <- readMgfData(mgf_file, verbose=T)\n",
    "    # myExp1 <- MSnbase::readMgfData(\"t.mgf\", verbose=T)\n",
    "    for (i in 1:ncol(fData(myExp1))) {\n",
    "      if (is.factor(fData(myExp1)[,i]))\n",
    "        fData(myExp1)[,i] <- as.character(fData(myExp1)[,i])\n",
    "    }\n",
    "    fData(myExp1)$spectrumFile <- mgf_file\n",
    "    allSpectra[[mgf_file]] <- myExp1 #updateFeatureNames(myExp1,label = paste(\"Sample\",i))\n",
    "  }\n",
    "\n",
    "  print(\"Adding identifications ...\")\n",
    "  \n",
    "  # myExp1 <- addIdentificationData(myExp1,psms,decoy=\"Decoy\",rank=\"rank\",acc=\"Protein.s.\",\n",
    "  #                                 icol=\"spectrumID\",fcol=\"spectrumId\",desc=\"desc\",pepseq=\"Modified.Sequence\",verbose=T)\n",
    "  cl <- makeCluster(4)\n",
    "  myExp <- qnt <- list()\n",
    "  for (mgf_file in mgf_files) {\n",
    "    myExp[[mgf_file]] <- addIdentificationData(allSpectra[[mgf_file]],psms,decoy=\"Decoy\",rank=\"rank\",acc=\"Protein.s.\",\n",
    "                                  icol=\"Spectrum.Title\",fcol=\"TITLE\",desc=\"desc\",pepseq=\"Modified.Sequence\",verbose=T)\n",
    "    print(idSummary(myExp[[mgf_file]]))\n",
    "    myExp[[mgf_file]] <- removeNoId(myExp[[mgf_file]])\n",
    "    qnt[[mgf_file]] <- quantify(myExp[[mgf_file]], method=\"sum\", reporters = quant.method, strict =F, verbose=T )\n",
    "  } \n",
    "\n",
    "  ## Convert SearchGUI output to file format that can be processed by isobar\n",
    "  stopCluster(cl)\n",
    "\n",
    "print(\"Adding reporters ...\")    \n",
    "    \n",
    "  imp<-makeImpuritiesMatrix(length(quant.method),edit=F)\n",
    "  for (i in 1:length(qnt)) {\n",
    "    qnt[[i]] <- purityCorrect(qnt[[i]],imp)\n",
    "    exprs(qnt[[i]]) <- log2(exprs(qnt[[i]]))\n",
    "    qnt[[i]] <- normalise(qnt[[i]],\"center.median\")\n",
    "    qnt[[i]] <- updateFeatureNames(qnt[[i]],label = paste(\"Sample\",i))\n",
    "  }\n",
    "      \n",
    "  names(qnt) <- NULL\n",
    "  allqnt <- do.call(\"combine\",args=qnt)\n",
    "  \n",
    "    \n",
    "  allqnt <- filterNA(allqnt, pNA=0.5)\n",
    "\n",
    "  ## Setting the stage for the iPQF inference method\n",
    "  names(fData(allqnt))[which(names(fData(allqnt))==\"Protein.s.\")] <- \"accession\"\n",
    "  names(fData(allqnt))[which(names(fData(allqnt))==\"Variable.Modifications\")] <- \"modifications\"\n",
    "  names(fData(allqnt))[which(names(fData(allqnt))==\"m.z\")] <- \"mass_to_charge\"\n",
    "  names(fData(allqnt))[which(names(fData(allqnt))==\"Confidence....\")] <- \"search_engine_score\"\n",
    "    \n",
    "  write.csv(exprs(allqnt),paste(\"/home/biodocker/OUT/\",s,\"/AllQuantPSMs.csv\",sep=\"\"))  \n",
    "    save(allqnt, file=paste(\"/home/biodocker/OUT/\",s,\"/AllQuantPSMs.RData\",sep=\"\") )\n",
    "    \n",
    "  ## Plots\n",
    "  boxplot(exprs(allqnt))\n",
    "\n",
    "panel.cor <- function(x, y, digits=2, prefix=\"\", cex.cor) \n",
    "{\n",
    "    usr <- par(\"usr\"); on.exit(par(usr)) \n",
    "    par(usr = c(0, 1, 0, 1)) \n",
    "    r <- abs(cor(x, y)) \n",
    "    txt <- format(c(r, 0.123456789), digits=digits)[1] \n",
    "    txt <- paste(prefix, txt, sep=\"\") \n",
    "    if(missing(cex.cor)) cex <- 0.8/strwidth(txt) \n",
    " \n",
    "    test <- cor.test(x,y) \n",
    "    # borrowed from printCoefmat\n",
    "    Signif <- symnum(test$p.value, corr = FALSE, na = FALSE, \n",
    "                  cutpoints = c(0, 0.001, 0.01, 0.05, 0.1, 1),\n",
    "                  symbols = c(\"***\", \"**\", \"*\", \".\", \" \")) \n",
    " \n",
    "    text(0.5, 0.5, txt, cex = cex * r) \n",
    "    text(.8, .8, Signif, cex=cex, col=2) \n",
    "}\n",
    "    \n",
    "  pairs(exprs(allqnt),lower.panel=panel.smooth, upper.panel=panel.cor)\n",
    "    \n",
    "  #print(fData(allqnt))  \n",
    "}\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in readChar(con, 5L, useBytes = TRUE) : cannot open the connection\n",
      "In addition: Warning message:\n",
      "In readChar(con, 5L, useBytes = TRUE) :\n",
      "  cannot open compressed file '/home/biodocker/OUT/./AllQuantPSMs.RData', probable reason 'No such file or directory'\n",
      "Error in readChar(con, 5L, useBytes = TRUE) : cannot open the connection\n"
     ]
    }
   ],
   "source": [
    "%%R\n",
    "\n",
    "for (s in samples) {\n",
    "\n",
    "    load(paste(\"/home/biodocker/OUT/\",s,\"/AllQuantPSMs.RData\",sep=\"\"))\n",
    "  exprs(allqnt) <- 2^exprs(allqnt)\n",
    "\n",
    "\n",
    "print(\"Protein inference ...\")    \n",
    "    \n",
    "    \n",
    "  # TODO: change to iPQF but still giving error  \n",
    "  allProts <- combineFeatures(allqnt, groupBy=fData(allqnt)$accession, fun=\"medpolish\",verbose=T)\n",
    "  exprs(allProts) <- log2(exprs(allProts))\n",
    "   \n",
    "    \n",
    "  print(head(exprs(allProts)))\n",
    "    \n",
    "  boxplot(exprs(allProts))\n",
    "    \n",
    "  #stop(\"\")\n",
    "  #pairs(exprs(allProts))\n",
    "  #save\n",
    "  write.exprs(allProts,file=paste(\"/home/biodocker/OUT/\",s,\"/AllQuantProteins.csv\",sep=\"\"))\n",
    "  \n",
    "  ProtDat[[s]] <- allProts <- exprs(allProts)\n",
    "  pca <- princomp(allProts[complete.cases((allProts)),])\n",
    "  plot(pca)\n",
    "  plot(pca$loadings)\n",
    "  text(pca$loadings,colnames(ProtDat))\n",
    "  \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Hide code",
  "hide_code_all_hidden": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "notify_time": "5"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
