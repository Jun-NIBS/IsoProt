{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "# Protocol for analysis of labeled proteomics data\n",
    "We recommend the following structure for describing the protocol and its example workflow. Below you can find an overview of required features of the protocol and its description and rules for best practices.\n",
    "\n",
    "This is the link to the github repository of the protocol \n",
    "TODO add version!\n",
    "https://github.com/ProtProtocols/biocontainer-jupyter\n",
    "\n",
    "Link to docker image:\n",
    "\n",
    "\n",
    "\n",
    "## Abstract\n",
    "\n",
    "Labelled peptide mass spectrometry provides fast large-scale comparison of protein abundances over multiple conditions. To date, no one stop shop software solution exists that enables the common researcher to carry out the full analysis of the acquired raw data. Pipeline for this analysis have often been established in laboratories that are based on a combination of different software tools and in-house programs. Different and often new versions of the used tools and issues with the compatibility of apparently interoperable tools make it very difficult to ensure reproducible proteomics data analysis. We present a 100% reproducible software protocol to fully analyze data from one of the most popular types of proteomics experiments. The protocol is fully based on open source tools installed on a docker container, additionally providing a user-friendly and interactive browser interface for guidance of configuration and execution of the different operations. An example use case is provided that can be used for testing and adaption of own data sets. With this setup, analysis of labelled MS data will yield identical results on any computer that meets the computational bandwidth to run the analysis. \n",
    "\n",
    "(Provide a short description of the software protocol including broader context, functionality, use case and purpose.)\n",
    "\n",
    "\n",
    "## Maintainer\n",
    "Provide details about the protocol maintainer (e.g. email address and/or github username)\n",
    "\n",
    "## Software\n",
    "Specify links for documentation and tutorials of used software, source code, publications and use cases. Detail versions of each used software. Alternatively, provide links to the software descriptions in https://bio.tools where this information is available.\n",
    "\n",
    "## Diagram\n",
    "Provide a simple diagram of functionality of the workflow/software. We recommend using controlled vocabularies for input/output data types and file formats as well as provided operation of the tool(s). You can use http://edamontology.org terms for the description.\n",
    "\n",
    "__TODO: example__\n",
    "\n",
    "## System requirements\n",
    "Fill in the following items:\n",
    "Required hard disk space for docker image, input and output files: \n",
    "\n",
    "Required memory: \n",
    "\n",
    "Recommmended number of threads: \n",
    "\n",
    "## Example \n",
    "Presentation of well-documented instructions and commands to run the example use case. Depending on the use case and the software, provide link(s) to open the web service incorportated in the Docker image (e.g. 0.0.0.0:8080), bash commands to run programs from the command line and additional code for e.g. checking and visualizing the (intermediate) results. \n",
    "\n",
    "Instead of providing the instructions in this notebook, one can also provide a link to a notebook containing the example use case.\n",
    "\n",
    "## More general use case (optional)\n",
    "Provide link to notebook with a generalized use case that easily can be adapted to e.g. process different input data and concurrent parametrization.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "# Example\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO list\n",
    "\n",
    "- Add possibility of samples that are arranged in different folders\n",
    "- Evaluate TMTc: https://pubs.acs.org/doi/10.1021/acs.analchem.7b04713\n",
    "- Protein inference function (can be iPQF)\n",
    "- Protein summarization function\n",
    "- Separate R scripts for quantification of psms, protein level stats and rest\n",
    "- Run scripts interactively through buttons\n",
    "- Save all configurations in file\n",
    "\n",
    "**Completed:**\n",
    "- Python script: Fix search with new PeptideShaker version\n",
    "- Python script: Adapt all MGF titles prior to search\n",
    "- Remove quotes from titles in mgf-files\n",
    "- Python script: Add MGF Peak filter function (100 - 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-06T12:27:01.281810Z",
     "start_time": "2018-04-06T12:27:00.218320Z"
    },
    "hideCode": true,
    "hideOutput": true,
    "hidePrompt": false,
    "hide_input": true,
    "init_cell": true,
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "%load_ext rpy2.ipython"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "Specify parameters for database search and evaluation of identified peptide-spectrum matches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-06T12:27:04.484391Z",
     "start_time": "2018-04-06T12:27:01.283376Z"
    },
    "code_folding": [
     6,
     106,
     167,
     222,
     407
    ],
    "hideCode": true,
    "hidePrompt": false,
    "hide_input": true,
    "init_cell": true,
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce22e42a35ad46369fab18ba61cee5d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>VBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "VBox(children=(Label(value='Working directory'), Dropdown(index=1, options={'/data/': '/data/', 'Example files': '/home/biodocker/'}, value='/home/biodocker/'), Label(value='Precursor tolerance (ppm):'), IntSlider(value=20, max=30, min=-10), Label(value='Fragment ion tolerance (da):'), BoundedFloatText(value=0.05, max=200.0), Label(value='Fasta file (database, must NOT contain decoy sequences):'), Dropdown(index=1, options={'IN/sp_human.fasta': '/home/biodocker/IN/sp_human.fasta', 'IN/sp_human_concatenated_target_decoy.fasta': '/home/biodocker/IN/sp_human_concatenated_target_decoy.fasta'}, value='/home/biodocker/IN/sp_human_concatenated_target_decoy.fasta'), Checkbox(value=True, description='Generate decoy sequences'), Label(value='Quantification method:'), Dropdown(index=3, options={'iTRAQ4': 'iTRAQ 4-plex of K,iTRAQ 4-plex of Y,iTRAQ 4-plex of peptide N-term', 'iTRAQ8 (variable)': 'iTRAQ 8-plex of Y', 'iTRAQ8 (fixed)': 'iTRAQ 8-plex of K, iTRAQ 8-plex of peptide N-term', 'TMT10': 'TMT 10-plex of K,TMT 10-plex of peptide N-term', 'TMT6': 'TMT 6-plex of K,TMT 6-plex of peptide N-term'}, value='TMT 10-plex of K,TMT 10-plex of peptide N-term'), Label(value='Number of miscleavages;'), IntSlider(value=1, max=10), Label(value='Further fixed modifications'), Dropdown(options=('Carbamidomethylation of C', 'None'), value='Carbamidomethylation of C'), Label(value='Further variable modifications (Hold Ctrl to select multiple)'), SelectMultiple(index=(0,), options=('Oxidation of M', 'Phosphorylation of STY', 'Acetylation of peptide N-term', 'Acetylation of protein N-term'), value=('Oxidation of M',)), Label(value='Folder for spectra files (files need to be mgf)'), Dropdown(index=2, options={'LOG': '/home/biodocker/LOG', 'misc': '/home/biodocker/misc', 'IN': '/home/biodocker/IN', 'OUT': '/home/biodocker/OUT', 'bin': '/home/biodocker/bin'}, value='/home/biodocker/IN'), Button(description='Run Search', icon='check', style=ButtonStyle(), tooltip='Launch the search')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adapting MGF titles...\n",
      "Extracting reporter peaks...\n",
      "Creating decoy database...\n",
      "Creating search parameter file...\n",
      "Running search...\n",
      "  Searching files in /home/biodocker/IN\n",
      "Search completed.\n",
      "Processing result using PeptideShaker...\n",
      "Fri Apr 06 14:53:43 UTC 2018 Unzipping searchgui_out.zip.\n",
      "10% 20% 30% 40% 50% 60% 70% 80% 90% 100%\n",
      "\n",
      "Fri Apr 06 14:53:43 UTC 2018 Import process for experiment1 (Sample: test, Replicate: 1)\n",
      "\n",
      "Fri Apr 06 14:53:44 UTC 2018 Importing sequences from sp_human_concatenated_target_decoy_concatenated_target_decoy.fasta.\n",
      "Reindexing: sp_human_concatenated_target_decoy_concatenated_target_decoy.fasta. (Reason: com.compomics.util.experiment.identification.protein_sequences.FastaIndex; local class incompatible: stream classdesc serialVersionUID = 6048982043770405466, local class serialVersionUID = 8022987705958686236)\n",
      "Reindexing: sp_human_concatenated_target_decoy_concatenated_target_decoy.fasta.\n",
      "10% 20% 30% 40% 50% 60% 70% 80% 90% 0%\n",
      "10% 20% 30% 40% 50% 60% 70% 80% 90% 100%\n",
      "Fri Apr 06 14:53:56 UTC 2018 FASTA file import completed.\n",
      "Fri Apr 06 14:53:56 UTC 2018 Importing gene mappings.\n",
      "Fri Apr 06 14:53:57 UTC 2018 Establishing local database connection.\n",
      "Fri Apr 06 14:54:03 UTC 2018 Reading identification files.\n",
      "Fri Apr 06 14:54:03 UTC 2018 Parsing .AllQuantPSMs.csv.\n",
      "Fri Apr 06 14:54:03 UTC 2018 Identification result file '.AllQuantPSMs.csv' not recognized.\n",
      "Fri Apr 06 14:54:03 UTC 2018 PeptideShaker Processing Canceled.\n",
      "<CompomicsError>PeptideShaker processing canceled. See the PeptideShaker log for details.</CompomicsError>\n",
      "\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Failed to run PeptideShaker",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-7a1239b233cd>\u001b[0m in \u001b[0;36mrun_search\u001b[0;34m(button)\u001b[0m\n\u001b[1;32m    370\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpeptide_shaker_process\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpeptide_shaker_process\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 372\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Failed to run PeptideShaker\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    373\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpeptide_shaker_result_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: Failed to run PeptideShaker"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adapting MGF titles...\n",
      "Extracting reporter peaks...\n",
      "Creating decoy database...\n",
      "Creating search parameter file...\n",
      "Running search...\n",
      "  Searching files in /home/biodocker/IN\n"
     ]
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from ipywidgets import VBox, Label\n",
    "import sys, os\n",
    "import subprocess\n",
    "\n",
    "# create an empty class as a storage for all UI widgets\n",
    "class SearchUI:\n",
    "    def __init__(self):\n",
    "        self.work_dir_select = widgets.Dropdown(options={'/data/': '/data/', 'Example files': '/home/biodocker/'}, \n",
    "                                                value='/home/biodocker/')\n",
    "        self.work_dir_select.observe(observe_work_dir_select)\n",
    "        \n",
    "        self.precursor_tolerance = widgets.IntSlider(min=-10,max=30,step=1,value=20)\n",
    "        self.fragment_tolerance = widgets.BoundedFloatText(min=0,max=200,value=0.05)\n",
    "        self.fasta_db = widgets.Dropdown(options={\"sp_human.fasta\": \"IN/sp_human.fasta\"})\n",
    "        \n",
    "        self.generate_decoy = widgets.Checkbox(value=True, description=\"Generate decoy sequences\")\n",
    "        \n",
    "        # TODO  needs table to describe labeling formats\n",
    "        self.labelling = widgets.Dropdown(options=\n",
    "                          {'TMT6': 'TMT 6-plex of K,TMT 6-plex of peptide N-term',\n",
    "                           'TMT10': 'TMT 10-plex of K,TMT 10-plex of peptide N-term','iTRAQ4': 'iTRAQ 4-plex of K,iTRAQ 4-plex of Y,iTRAQ 4-plex of peptide N-term',\n",
    "                           'iTRAQ8 (fixed)': 'iTRAQ 8-plex of K, iTRAQ 8-plex of peptide N-term',\n",
    "                           'iTRAQ8 (variable)': 'iTRAQ 8-plex of Y'},\n",
    "                      value='TMT 10-plex of K,TMT 10-plex of peptide N-term')\n",
    "        \n",
    "        self.missed_cleavages = widgets.IntSlider(min=0,max=10,step=1,value=1)\n",
    "        self.fixed_ptms = widgets.Dropdown(options=[\"Carbamidomethylation of C\",\"None\"])\n",
    "\n",
    "        # PTMs\n",
    "        self.var_ptms = widgets.SelectMultiple(\n",
    "            options=[\"Oxidation of M\",\n",
    "                     \"Phosphorylation of STY\",\n",
    "                     \"Acetylation of peptide N-term\",\n",
    "                     \"Acetylation of protein N-term\"],\n",
    "            value=['Oxidation of M'])\n",
    "        \n",
    "        self.spectra_dir = widgets.Dropdown(options={\"IN\": \"IN\"})\n",
    "\n",
    "        # ww = widgets.Checkbox(description=\"Decoy\")\n",
    "\n",
    "        self.search_button = widgets.Button(\n",
    "            description='Run Search',\n",
    "            disabled=False,\n",
    "            button_style='', # 'success', 'info', 'warning', 'danger' or ''\n",
    "            tooltip='Launch the search',\n",
    "            icon='check'\n",
    "        )\n",
    "\n",
    "        self.search_button.on_click(run_search)\n",
    "        \n",
    "        \n",
    "    def updateFastaFiles(self, workdir):\n",
    "        # get all FASTA files\n",
    "        fasta_files = [file for file in os.listdir(workdir) if file[-6:] == \".fasta\"]\n",
    "        \n",
    "        # also search all subdirectories for FASTA files\n",
    "        for d in os.listdir(workdir):\n",
    "            d_path = os.path.join(workdir, d)\n",
    "            if os.path.isdir(d_path) and d[0] != \".\":\n",
    "                fasta_files += [os.path.join(d, file) for file in os.listdir(d_path) if file[-6:] == \".fasta\"]\n",
    "        \n",
    "        # create the dict to add as values to the control\n",
    "        file_list = dict()\n",
    "        sel_value = None\n",
    "        \n",
    "        for f in fasta_files:\n",
    "            file_list[f] = os.path.join(os.path.abspath(workdir), f)\n",
    "            if sel_value is None:\n",
    "                sel_value = os.path.join(os.path.abspath(workdir), f)\n",
    "        \n",
    "        self.fasta_db.options = file_list\n",
    "        self.fasta_db.value = sel_value\n",
    "        \n",
    "        # update the list of possible peaklist directories\n",
    "        directories = [d for d in os.listdir(workdir) if os.path.isdir(os.path.join(workdir, d)) and d[0] != \".\"]\n",
    "        \n",
    "        dir_list = dict()\n",
    "        \n",
    "        for d in directories:\n",
    "            dir_list[d] = os.path.join(os.path.abspath(workdir), d)\n",
    "            \n",
    "        self.spectra_dir.options = dir_list\n",
    "        \n",
    "        if \"IN\" in dir_list:\n",
    "            self.spectra_dir.value = dir_list[\"IN\"]\n",
    "        \n",
    "        \n",
    "    def display(self):\n",
    "        self.updateFastaFiles(self.work_dir_select.value)\n",
    "        \n",
    "        settings_box = VBox([Label('Working directory'), self.work_dir_select,\n",
    "                             Label('Precursor tolerance (ppm):'), self.precursor_tolerance, \n",
    "                             Label('Fragment ion tolerance (da):'), self.fragment_tolerance,\n",
    "                             Label('Fasta file (database, must NOT contain decoy sequences):'), self.fasta_db,\n",
    "                             self.generate_decoy,\n",
    "                             Label('Quantification method:'), self.labelling,\n",
    "                             Label('Number of miscleavages;'), self.missed_cleavages,\n",
    "                             Label('Further fixed modifications'), self.fixed_ptms,\n",
    "                             Label('Further variable modifications (Hold Ctrl to select multiple)'), self.var_ptms,\n",
    "                             Label('Folder for spectra files (files need to be mgf)'), self.spectra_dir,\n",
    "                             self.search_button])\n",
    "\n",
    "        display(settings_box)\n",
    "        \n",
    "\n",
    "class ExpDesignUI:\n",
    "    def __init__(self, labelling_technique):\n",
    "        \"\"\"\n",
    "        Generates all use interface objects as member variables.\n",
    "        \n",
    "        :param labelling_technique: The labelling method used.\n",
    "        \"\"\"\n",
    "        # always expect two groups\n",
    "        self.group1_name = widgets.Text(placeholder = \"Treatment\", description = \"Group 1:\")\n",
    "        self.group2_name = widgets.Text(placeholder = \"Control\", description = \"Group 2:\")\n",
    "        \n",
    "        self.channels = {\n",
    "            'TMT6': [\"126\", \"127\", \"128\", \"129\", \"130\", \"131\"],\n",
    "            'TMT10': [\"126\", \"127N\", \"127C\", \"128N\", \"128C\", \"129N\", \"129C\", \"130N\", \"130C\", \"131\"],\n",
    "            'iTRAQ8': [\"113\", \"114\", \"115\", \"116\", \"117\", \"118\", \"119\", \"121\"]\n",
    "        }\n",
    "        \n",
    "        if labelling_technique not in self.channels:\n",
    "            raise Exception(\"Unknown labelling technique: '\" + labelling_technique + \"'\")\n",
    "            \n",
    "        self.labelling_technique = labelling_technique\n",
    "            \n",
    "        # generate the textfields for the channels\n",
    "        self.channel_names = list()\n",
    "        \n",
    "        for channel in self.channels[self.labelling_technique]:\n",
    "            self.channel_names.append(widgets.Text(description = channel, placeholder = \"Sample \" + channel))\n",
    "            \n",
    "        # add select boxes to select the experimental group\n",
    "        self.group_selects = list()\n",
    "        \n",
    "        for channel in self.channels[self.labelling_technique]:\n",
    "            self.group_selects.append(widgets.Dropdown(options = [\"Group 1\", \"Group 2\"], value = \"Group 1\"))\n",
    "            \n",
    "        self.save_button = widgets.Button(\n",
    "            description='Save design',\n",
    "            disabled=False,\n",
    "            button_style='', # 'success', 'info', 'warning', 'danger' or ''\n",
    "            tooltip='Save the experimental design',\n",
    "            icon='check'\n",
    "        )\n",
    "\n",
    "        self.save_button.on_click(self.save_design)\n",
    "\n",
    "    def display(self):\n",
    "        widget_list = [widgets.Label(\"Treatment group names:\"), self.group1_name, self.group2_name,\n",
    "                       widgets.Label(\"Sample names (per channel):\")]\n",
    "        \n",
    "        for i in range(0, len(self.channel_names)):\n",
    "            widget_list.append(widgets.HBox([self.channel_names[i], self.group_selects[i]]))\n",
    "            \n",
    "        widget_list.append(self.save_button)\n",
    "        \n",
    "        widget_box = VBox(widget_list)\n",
    "        \n",
    "        display(widget_box)\n",
    "        \n",
    "    def save_design(self):\n",
    "        pass\n",
    "      \n",
    "        \n",
    "def adapt_mgf_titles(filenames):\n",
    "    \"\"\"\n",
    "    This function changes all MGF titles to [filename].[spec index]\n",
    "    :param: filenames: Filenames of the MGF files to change\n",
    "    \"\"\"\n",
    "    for filename in filenames:\n",
    "        with open(filename, \"r\") as reader:\n",
    "            clean_name = os.path.basename(filename).replace(\" \", \"_\")\n",
    "            # MGF index reference in PSI standard is 1-based\n",
    "            cur_index = 1\n",
    "            \n",
    "            with open(filename + \".tmp\", \"w\") as writer:\n",
    "                for line in reader:\n",
    "                    if line[0:6] == \"TITLE=\":\n",
    "                        writer.write(\"TITLE=\" + clean_name + \".\" + str(cur_index) + \"\\n\")\n",
    "                        cur_index += 1\n",
    "                    else:\n",
    "                        writer.write(line)\n",
    "                        \n",
    "        # backup the original file\n",
    "        os.rename(filename, filename + \".org\")\n",
    "        os.rename(filename + \".tmp\", filename)\n",
    "        \n",
    "        \n",
    "def filter_mgf_peaks(filenames, min_mz=100, max_mz=150):\n",
    "    \"\"\"\n",
    "    Removes all peaks from the passed mgf files that are below min_mz or\n",
    "    above max_mz. The results are written to files with the same name but\n",
    "    \".filtered\" appended to the name.\n",
    "    :param: filenames: List of filenames to process.\n",
    "    :param: min_mz: Minimum m/z a peak must have to be kept\n",
    "    :param: max_mz: Maximum m/z a peak may have to be kept\n",
    "    \"\"\"\n",
    "    for filename in filenames:\n",
    "        with open(filename, \"r\") as reader:\n",
    "            with open(filename + \".filtered\", \"w\") as writer:\n",
    "                for line in reader:\n",
    "                    # check if it's a peak\n",
    "                    if line[0].isdigit():\n",
    "                        sep_index = line.find(\" \")\n",
    "                        if sep_index < 0:\n",
    "                            sep_index = line.find(\"\\t\")\n",
    "                        if sep_index < 0:\n",
    "                            raise Exception(\"Invalid peak definition found: \" + line + \n",
    "                                            \". Failed to filter file \" + filename)\n",
    "                            \n",
    "                        mz = float(line[:sep_index])\n",
    "                            \n",
    "                        # ignore any non-matching peaks\n",
    "                        if mz < min_mz or mz > max_mz:\n",
    "                            continue\n",
    "                            \n",
    "                    # copy the line\n",
    "                    writer.write(line)\n",
    "    \n",
    "        \n",
    "def run_search(button):\n",
    "    global searchUI, result_file\n",
    "    \n",
    "    # make sure all required fields were selected\n",
    "    if searchUI.work_dir_select.value is None:\n",
    "        print(\"Error: No working directory selected\")\n",
    "        return\n",
    "    \n",
    "    if searchUI.fasta_db.value is None:\n",
    "        print(\"Error: No FASTA file selected\")\n",
    "        return\n",
    "    \n",
    "    # create the directory paths to work in\n",
    "    peaklist_dir = os.path.abspath(searchUI.spectra_dir.value)\n",
    "    \n",
    "    if not os.path.isdir(peaklist_dir):\n",
    "        raise Exception(\"Invalid peak list directory selected: \" + peaklist_dir + \" does not exist.\")\n",
    "    \n",
    "    peptide_shaker_jar = \"/home/biodocker/bin/PeptideShaker-1.16.17/PeptideShaker-1.16.17.jar\"\n",
    "    searchgui_jar = \"/home/biodocker/bin/SearchGUI-3.2.20/SearchGUI-3.2.20.jar\"\n",
    "    work_dir = os.path.abspath(os.path.join(searchUI.work_dir_select.value, \"OUT\"))\n",
    "    \n",
    "    # the searches should be performed in the \"OUT\" directory\n",
    "    if not os.path.isdir(work_dir):\n",
    "        os.mkdir(work_dir)\n",
    "        \n",
    "    \n",
    "    # -------------------------------------\n",
    "    # Fix all MGF titles\n",
    "    print(\"Adapting MGF titles...\")\n",
    "    mgf_filenames = [os.path.join(peaklist_dir, f) for f in os.listdir(peaklist_dir) if f[-4:].lower() == \".mgf\"]\n",
    "    adapt_mgf_titles(mgf_filenames)\n",
    "    \n",
    "    print(\"Extracting reporter peaks...\")\n",
    "    filter_mgf_peaks(mgf_filenames)\n",
    "    \n",
    "        \n",
    "    # -------------------------------------\n",
    "    # Generate the decoy database\n",
    "    \n",
    "    if searchUI.generate_decoy.value == True:\n",
    "        print(\"Creating decoy database...\")\n",
    "\n",
    "        # create the decoy database\n",
    "        subprocess.run([\"java\", \"-cp\", searchgui_jar, \n",
    "                       \"eu.isas.searchgui.cmd.FastaCLI\", \"-in\", searchUI.fasta_db.value, \"-decoy\"], check=True,\n",
    "                       cwd=work_dir,\n",
    "                       stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "\n",
    "        # get the filename of the decoy database\n",
    "        database_file = os.path.abspath(searchUI.fasta_db.value)[:-6] + \"_concatenated_target_decoy.fasta\"\n",
    "    else:\n",
    "        # simply use the selected database file\n",
    "        database_file = os.path.abspath(searchUI.fasta_db.value)\n",
    "    \n",
    "    if not os.path.isfile(database_file):\n",
    "        raise Exception(\"Failed to find generated decoy database\")\n",
    "        \n",
    "    # ---------------------------------------------\n",
    "    # Create the search parameter file\n",
    "        \n",
    "    # build the arguments to create the parameter file\n",
    "    param_file = os.path.join(work_dir, \"search.par\")\n",
    "    \n",
    "    # remove any old parameters\n",
    "    if os.path.isfile(param_file):\n",
    "        os.remove(param_file)\n",
    "    \n",
    "    search_args = [\"java\", \"-cp\", searchgui_jar,\n",
    "                   \"eu.isas.searchgui.cmd.IdentificationParametersCLI\",\n",
    "                   \"-out\", param_file]\n",
    "    \n",
    "    # precursor tolerance\n",
    "    search_args.append(\"-prec_tol\")\n",
    "    search_args.append(str(searchUI.precursor_tolerance.value))\n",
    "    # fragment tolerance\n",
    "    search_args.append(\"-frag_tol\")\n",
    "    search_args.append(str(searchUI.fragment_tolerance.value))\n",
    "    # fixed mods\n",
    "    # TODO: labelling cannot always be set as fixed mod???\n",
    "    fixed_mod_string = str(searchUI.labelling.value) + \",\" + str(searchUI.fixed_ptms.value)\n",
    "    search_args.append(\"-fixed_mods\")\n",
    "    search_args.append(fixed_mod_string)\n",
    "    # database\n",
    "    search_args.append(\"-db\")\n",
    "    search_args.append(database_file)\n",
    "    # missed cleavages\n",
    "    search_args.append(\"-mc\")\n",
    "    search_args.append(str(searchUI.missed_cleavages.value))\n",
    "    \n",
    "    # var mods\n",
    "    if len(searchUI.var_ptms.value) > 0:\n",
    "        search_args.append(\"-variable_mods\")\n",
    "        var_mod_list = list()\n",
    "        \n",
    "        for var_mod in searchUI.var_ptms.value:\n",
    "            if var_mod == \"Phosphorylation of STY\":\n",
    "                var_mod_list += [\"Phosphorylation of S\", \"Phosphorylation of T\", \"Phosphorylation of Y\"]\n",
    "            else:\n",
    "                var_mod_list.append(var_mod)\n",
    "                \n",
    "        search_args.append(\",\".join(var_mod_list))\n",
    "        \n",
    "    # create the search parameter file\n",
    "    print(\"Creating search parameter file...\")\n",
    "    # print(\" \".join(search_args))\n",
    "    subprocess.run(search_args, check=True, cwd=work_dir, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    \n",
    "    if not os.path.isfile(param_file):\n",
    "        raise Exception(\"Failed to create search parameters\")\n",
    "        \n",
    "    # ------------------------------------------------\n",
    "    # Run the search\n",
    "    print(\"Running search...\")\n",
    "    # TODO: create list of spectrum files - or the folder\n",
    "    spectrum_files = peaklist_dir\n",
    "    print(\"  Searching files in \" + spectrum_files)\n",
    "    search_process = subprocess.run([\"java\", \"-cp\", searchgui_jar,\n",
    "                    \"eu.isas.searchgui.cmd.SearchCLI\", \"-spectrum_files\", spectrum_files,\n",
    "                    \"-output_folder\", work_dir, \"-id_params\", param_file,\n",
    "                    \"-xtandem\", \"0\", \"-msgf\", \"1\", \"-comet\", \"0\", \"-ms_amanda\", \"0\", \n",
    "                    \"-myrimatch\", \"0\", \"-andromeda\", \"0\", \"-omssa\", \"0\", \"-tide\", \"0\"],\n",
    "                    check=False, cwd=work_dir, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, universal_newlines=True)\n",
    "    \n",
    "    if search_process.returncode != 0:\n",
    "        print(search_process.stdout)\n",
    "        raise Exception(\"Search process failed.\")\n",
    "    \n",
    "    print(\"Search completed.\")\n",
    "    \n",
    "    # -------------------------------------------------\n",
    "    # Run PeptideShaker\n",
    "    print(\"Processing result using PeptideShaker...\")\n",
    "    peptide_shaker_result_file = os.path.join(work_dir, \"experiment.cpsx\")\n",
    "\n",
    "    peptide_shaker_process = subprocess.run([\"java\", \"-cp\", peptide_shaker_jar,\n",
    "                    \"eu.isas.peptideshaker.cmd.PeptideShakerCLI\",\n",
    "                    \"-experiment\", \"experiment1\",\n",
    "                    \"-sample\", \"test\",\n",
    "                    \"-replicate\", \"1\",\n",
    "                    \"-identification_files\", work_dir,\n",
    "                    \"-out\", peptide_shaker_result_file,\n",
    "                    \"-id_params\", param_file,\n",
    "                    \"-spectrum_files\", spectrum_files],\n",
    "                    check=False, cwd=work_dir, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, universal_newlines=True)\n",
    "    \n",
    "    if peptide_shaker_process.returncode != 0:\n",
    "        print(peptide_shaker_process.stdout)\n",
    "        raise Exception(\"Failed to run PeptideShaker\")\n",
    "\n",
    "    if not os.path.isfile(peptide_shaker_result_file):\n",
    "        raise Exception(\"Failed to process result file.\")\n",
    "      \n",
    "    # ---------------------------------------------------\n",
    "    # create TSV output files\n",
    "    print(\"Converting result to TSV format...\")\n",
    "    conversion_process = subprocess.run([\"java\", \"-cp\", peptide_shaker_jar,\n",
    "                  \"eu.isas.peptideshaker.cmd.ReportCLI\",\n",
    "                  \"-in\", peptide_shaker_result_file,\n",
    "                  \"-out_reports\", work_dir,\n",
    "                  \"-reports\", \"8\"],\n",
    "                  check=False, cwd=work_dir, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, universal_newlines=True)\n",
    "    \n",
    "    if conversion_process.returncode != 0:\n",
    "        print(conversion_process.stdout)\n",
    "        raise Exception(\"Conversion process failed\")\n",
    "    \n",
    "    result_file=os.path.join(work_dir, \"experiment1_test_1_Extended_PSM_Report.txt\")\n",
    "    \n",
    "    if not os.path.isfile(result_file):\n",
    "        raise Exception(\"Error: Conversion failed\")\n",
    "        \n",
    "    print(\"Done.\")\n",
    "    \n",
    "    # -----------------------------------------------------\n",
    "    # Create the experimental design\n",
    "    labelling_method = list(searchUI.labelling.options.keys())[searchUI.labelling.index]\n",
    "    \n",
    "    if labelling_method[0:6] == \"iTRAQ8\":\n",
    "        labelling_method = \"iTRAQ8\"\n",
    "        \n",
    "    expDesignUI = ExpDesignUI(labelling_method)\n",
    "    expDesignUI.display()\n",
    "    \n",
    "    \n",
    "def observe_work_dir_select(change):\n",
    "    global work_dir_select\n",
    "    if change['type'] == \"change\" and change['name'] == \"value\":\n",
    "        # update the required UI controls\n",
    "        searchUI.updateFastaFiles(change[\"new\"])\n",
    "\n",
    "# -------------------\n",
    "# Code to create the UI\n",
    "# --------------------\n",
    "result_file=None\n",
    "work_dir=None\n",
    "peaklist_dir=os.path.abspath(\"IN\")\n",
    "\n",
    "searchUI = SearchUI()\n",
    "searchUI.display()\n",
    "\n",
    "# create parameter list as input for R script\n",
    "Rinput = [searchUI.labelling.value, searchUI.spectra_dir.value]\n",
    "\n",
    "#TODO set names of samples and replicates (peptideshaker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-06T13:36:36.444443Z",
     "start_time": "2018-04-06T13:36:36.427255Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4de969fda9a48aead83400e33ae486f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>Button</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "Button(description='Run quantification and peptide inference', layout=Layout(width='30%'), style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.execute_cell_range(IPython.notebook.get_selected_index()+1, IPython.notebook.get_selected_index()+2)"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.execute_cell_range(IPython.notebook.get_selected_index()+1, IPython.notebook.get_selected_index()+2)"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Javascript, display\n",
    "from ipywidgets import widgets, Layout\n",
    "\n",
    "def run_all(ev):\n",
    "    display(Javascript('IPython.notebook.execute_cell_range(IPython.notebook.get_selected_index()+1, IPython.notebook.get_selected_index()+2)'))\n",
    "\n",
    "button = widgets.Button(description=\"Run quantification and peptide inference\",layout=Layout(width='30%'))\n",
    "button.on_click(run_all)\n",
    "display(button)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-06T13:36:52.780952Z",
     "start_time": "2018-04-06T13:36:52.770331Z"
    },
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%R -i Rinput\n",
    "\n",
    "\n",
    "# reading parameters\n",
    "labeling <- Rinput[[1]]\n",
    "inputDir <- Rinput[[2]]\n",
    "\n",
    "# library causes the execution to fail if the library is missing\n",
    "suppressWarnings(suppressMessages(library(lattice)))\n",
    "suppressWarnings(suppressMessages(library(stringr)))\n",
    "suppressWarnings(suppressMessages(library(mzID)))\n",
    "#suppressWarnings(suppressMessages(library(matrixStats)))\n",
    "#suppressWarnings(suppressMessages(library(venneuler)))\n",
    "suppressWarnings(suppressMessages(library(MSnbase)))\n",
    "\n",
    "# IMPORTANT\n",
    "# remove quotes from mgf-files before conversion:\n",
    "#for file in *mgf; do sed 's/\\\"//g' $file > ${file%.*}_noquote.mgf; done\n",
    "#for file in *_noquote.mgf; do /usr/local/tpp/bin/msconvert \"$file\" \"${file%.*}.mzML\"; done \n",
    "# Add option to provide mzML directly\n",
    "\n",
    "print(inputDir)\n",
    "setwd(inputDir)\n",
    "\n",
    "## Folder names for different runs (e.g. different replicates, ...)\n",
    "samples <- s <- c(\".\")\n",
    "ProtDat <- list()\n",
    "for (s in samples) {\n",
    "  \n",
    "  ## Organize technical runs in the same folder\n",
    "  # filenames\n",
    "  ident_files <- list.files(paste(\"/home/biodocker/OUT/\",s,sep=\"\"),pattern=\"Extended_PSM_Report.txt$\",full.names=T)\n",
    "  #mzML_files <- list.files(s,pattern=\".mzML$\",full.names = T)\n",
    "  #mgf_files <- list.files(s,pattern=\"_noquote.mgf$\",full.names = T)\n",
    "  mgf_files <- list.files(s,pattern=\"mgf$\",full.names = T)\n",
    "  \n",
    "  # TODO load parameters \"%%R -i parname\"\n",
    "  # process the input files\n",
    "  max.fdr <- 0.01\n",
    "  \n",
    "  # translate into labeling method descriptors\n",
    "  quant.methods <- cbind(c(\"TMT 10-plex of K,TMT 10-plex of peptide N-term\",\"TMT 6-plex of K,TMT 6-plex of peptide N-term\",\n",
    "                           \"iTRAQ 4-plex of K,iTRAQ 4-plex of Y,iTRAQ 4-plex of peptide N-term\",\n",
    "                           \"iTRAQ 8-plex of K, iTRAQ 8-plex of peptide N-term\",\"iTRAQ 8-plex of Y\"),\n",
    "                         c(\"TMT10\",\"TMT6\",\"iTRAQ4\",\"iTRAQ8\",\"iTRAQ8\"))\n",
    "    if(is.null(quant.methods[quant.methods[,1] == labeling,2])) {\n",
    "        stop(\"Error: labeling method not available\")\n",
    "    }\n",
    "  quant.method <- get(quant.methods[quant.methods[,1] == labeling,2])\n",
    "  \n",
    "    \n",
    "    \n",
    "   ## TODO read from experimental design\n",
    "  class.labels <- c(\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \"J\")\n",
    "  args <- commandArgs(trailingOnly = TRUE)\n",
    "  \n",
    "  if (is.null(ident_files)) {\n",
    "    stop(\"Error: No identification files\")\n",
    "  }\n",
    "\n",
    "      if (is.null(mgf_files)) {\n",
    "    stop(\"Error: No spectrum files\")\n",
    "  }\n",
    "\n",
    "    \n",
    "  if (!file.exists(ident_files)) {\n",
    "    stop(\"Error: Cannot find identification files \", ident_files)\n",
    "  }\n",
    "  for (mgf_file in mgf_files) {\n",
    "    if (!file.exists(mgf_file)) {\n",
    "      stop(\"Error: Cannot find mgf file \", mgf_file)\n",
    "    }\n",
    "  }\n",
    "  \n",
    "  # Convert SearchGUI output to isobar format\n",
    "  psms <- read.csv(ident_files, sep = \"\\t\",stringsAsFactors = F)\n",
    "  \n",
    "  if (! \"Decoy\" %in% names(psms)) {\n",
    "    stop(\"Error: No decoy information available in output file\")\n",
    "  }\n",
    "  \n",
    "  print(paste(\"Loaded\",nrow(psms), \"PSMs\"))\n",
    "  \n",
    "  # ---- Confidence filter ----\n",
    "  psms <- psms[order(psms[, \"Confidence....\"], decreasing = T), ]\n",
    "  decoy.psms <- which(psms[, \"Decoy\"] == \"1\")\n",
    "  \n",
    "  decoy.count <- 0\n",
    "  \n",
    "  for (decoy.index in decoy.psms) {\n",
    "    decoy.count <- decoy.count + 1\n",
    "    target.count <- decoy.index - decoy.count\n",
    "    \n",
    "    cur.fdr <- (decoy.count * 2) / (decoy.count + target.count)\n",
    "    \n",
    "    if (cur.fdr > max.fdr) {\n",
    "      # filter\n",
    "      psms <- psms[1:decoy.index - 1,]\n",
    "      break\n",
    "    }\n",
    "  }\n",
    "  \n",
    "print(paste0(\"Filtered \", nrow(psms), \" PSMs @ \", max.fdr, \" FDR\"))\n",
    "    \n",
    "    #print(head(psms))\n",
    "    \n",
    "  #### prepare for MSnbase\n",
    "  psms$rank <- 1\n",
    "  psms$desc <- psms$Protein.s.\n",
    "  psms$spectrumID <- psms$Spectrum.Title #str_extract(psms$Spectrum.Title, \"scan=[0-9]*\")\n",
    "  psms$spectrumFile <- psms$Spectrum.File\n",
    "  psms$idFile <- ident_files\n",
    "  # remove unnecessary PTMs from modified sequence\n",
    "  # TODO: define whether to take oxidation, ...\n",
    "  psms$Modified.Sequence <- gsub(\"<cmm>\",\"\",psms$Modified.Sequence)\n",
    "  psms$Modified.Sequence <- sub(\"[a-z,A-Z]*-\",\"\",psms$Modified.Sequence)\n",
    "  psms$Modified.Sequence <- sub(\"-[a-z,A-Z]*\",\"\",psms$Modified.Sequence)\n",
    "  psms$Modified.Sequence <- gsub(\"<iTRAQ>\",\"\",psms$Modified.Sequence)\n",
    "  psms$Modified.Sequence <- gsub(\"<TMT>\",\"\",psms$Modified.Sequence)\n",
    "  psms$sequence <- psms$Modified.Sequence\n",
    "    #Optional:\n",
    "  # psms$Modified.Sequence <- gsub(\"<ox>\",\"\",psms$Modified.Sequence)\n",
    "  \n",
    "  \n",
    "  \n",
    "  # reading MS and identification data\n",
    "  allSpectra <- list()\n",
    "  for (mgf_file in mgf_files) {\n",
    "    # myExp1 <- readMSData(mzML_files, mode=\"onDisk\", verbose=T)\n",
    "    \n",
    "    myExp1 <- readMgfData(mgf_file, verbose=T)\n",
    "    # myExp1 <- MSnbase::readMgfData(\"t.mgf\", verbose=T)\n",
    "    for (i in 1:ncol(fData(myExp1))) {\n",
    "      if (is.factor(fData(myExp1)[,i]))\n",
    "        fData(myExp1)[,i] <- as.character(fData(myExp1)[,i])\n",
    "    }\n",
    "    fData(myExp1)$spectrumFile <- mgf_file\n",
    "    allSpectra[[mgf_file]] <- myExp1 #updateFeatureNames(myExp1,label = paste(\"Sample\",i))\n",
    "  }\n",
    "\n",
    "  print(\"Adding identifications ...\")\n",
    "  \n",
    "  # myExp1 <- addIdentificationData(myExp1,psms,decoy=\"Decoy\",rank=\"rank\",acc=\"Protein.s.\",\n",
    "  #                                 icol=\"spectrumID\",fcol=\"spectrumId\",desc=\"desc\",pepseq=\"Modified.Sequence\",verbose=T)\n",
    "  cl <- makeCluster(4)\n",
    "  myExp <- qnt <- list()\n",
    "  for (mgf_file in mgf_files) {\n",
    "    myExp[[mgf_file]] <- addIdentificationData(allSpectra[[mgf_file]],psms,decoy=\"Decoy\",rank=\"rank\",acc=\"Protein.s.\",\n",
    "                                  icol=\"Spectrum.Title\",fcol=\"TITLE\",desc=\"desc\",pepseq=\"Modified.Sequence\",verbose=T)\n",
    "    print(idSummary(myExp[[mgf_file]]))\n",
    "    myExp[[mgf_file]] <- removeNoId(myExp[[mgf_file]])\n",
    "    qnt[[mgf_file]] <- quantify(myExp[[mgf_file]], method=\"sum\", reporters = quant.method, strict =F, verbose=T )\n",
    "  } \n",
    "\n",
    "  ## Convert SearchGUI output to file format that can be processed by isobar\n",
    "  stopCluster(cl)\n",
    "\n",
    "print(\"Adding reporters ...\")    \n",
    "    \n",
    "  imp<-makeImpuritiesMatrix(length(quant.method),edit=F)\n",
    "  for (i in 1:length(qnt)) {\n",
    "    qnt[[i]] <- purityCorrect(qnt[[i]],imp)\n",
    "    exprs(qnt[[i]]) <- log2(exprs(qnt[[i]]))\n",
    "    qnt[[i]] <- normalise(qnt[[i]],\"center.median\")\n",
    "    qnt[[i]] <- updateFeatureNames(qnt[[i]],label = paste(\"Sample\",i))\n",
    "  }\n",
    "      \n",
    "  names(qnt) <- NULL\n",
    "  allqnt <- do.call(\"combine\",args=qnt)\n",
    "  \n",
    "    \n",
    "  allqnt <- filterNA(allqnt, pNA=0.5)\n",
    "\n",
    "  ## Setting the stage for the iPQF inference method\n",
    "  names(fData(allqnt))[which(names(fData(allqnt))==\"Protein.s.\")] <- \"accession\"\n",
    "  names(fData(allqnt))[which(names(fData(allqnt))==\"Variable.Modifications\")] <- \"modifications\"\n",
    "  names(fData(allqnt))[which(names(fData(allqnt))==\"m.z\")] <- \"mass_to_charge\"\n",
    "  names(fData(allqnt))[which(names(fData(allqnt))==\"Confidence....\")] <- \"search_engine_score\"\n",
    "    \n",
    "  write.csv(exprs(allqnt),paste(\"/home/biodocker/OUT/\",s,\"/AllQuantPSMs.csv\",sep=\"\"))  \n",
    "    save(allqnt, file=paste(\"/home/biodocker/OUT/\",s,\"/AllQuantPSMs.RData\",sep=\"\") )\n",
    "    \n",
    "  ## Plots\n",
    "  boxplot(exprs(allqnt))\n",
    "\n",
    "panel.cor <- function(x, y, digits=2, prefix=\"\", cex.cor) \n",
    "{\n",
    "    usr <- par(\"usr\"); on.exit(par(usr)) \n",
    "    par(usr = c(0, 1, 0, 1)) \n",
    "    r <- abs(cor(x, y)) \n",
    "    txt <- format(c(r, 0.123456789), digits=digits)[1] \n",
    "    txt <- paste(prefix, txt, sep=\"\") \n",
    "    if(missing(cex.cor)) cex <- 0.8/strwidth(txt) \n",
    " \n",
    "    test <- cor.test(x,y) \n",
    "    # borrowed from printCoefmat\n",
    "    Signif <- symnum(test$p.value, corr = FALSE, na = FALSE, \n",
    "                  cutpoints = c(0, 0.001, 0.01, 0.05, 0.1, 1),\n",
    "                  symbols = c(\"***\", \"**\", \"*\", \".\", \" \")) \n",
    " \n",
    "    text(0.5, 0.5, txt, cex = cex * r) \n",
    "    text(.8, .8, Signif, cex=cex, col=2) \n",
    "}\n",
    "    \n",
    "  pairs(exprs(allqnt),lower.panel=panel.smooth, upper.panel=panel.cor)\n",
    "    \n",
    "  #print(fData(allqnt))  \n",
    "}\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1] \"Protein inference ...\"\n",
       "Final: 0\n",
       "1: 2.377855\n",
       "2: 2.336358\n",
       "Final: 2.336358\n",
       "Final: 0\n",
       "Final: 0\n",
       "              126       127N       127C       128N       128C        129N\n",
       "P22033 -0.7395818 -0.5356091  0.0000000  0.0000000 -0.4217496 -0.11054132\n",
       "P25942 -0.6104500 -0.5356621  0.3954082  0.4955047  0.1609659  0.72642307\n",
       "P46776  0.2711442  0.2688829  0.9751971  0.7740157  0.0000000 -0.02387773\n",
       "Q9BQE5  0.5468595  0.1797187 -0.1783808 -0.2358337  0.4854915  0.50150400\n",
       "             129C        130N       130C         131\n",
       "P22033 -0.9196559 -0.42030346 -0.3691201 -0.42805730\n",
       "P25942 -0.7028576 -0.47582856 -0.2771131 -0.23393877\n",
       "P46776  0.3443143  0.39959382  0.2716429  0.00000000\n",
       "Q9BQE5  0.2711222  0.08659584  0.0000000  0.02781772\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%R\n",
    "\n",
    "for (s in samples) {\n",
    "\n",
    "    load(paste(\"/home/biodocker/OUT/\",s,\"AllQuantPSMs.RData\",sep=\"\"))\n",
    "  exprs(allqnt) <- 2^exprs(allqnt)\n",
    "\n",
    "\n",
    "print(\"Protein inference ...\")    \n",
    "    \n",
    "    \n",
    "  # TODO: change to iPQF but still giving error  \n",
    "  allProts <- combineFeatures(allqnt, groupBy=fData(allqnt)$accession, fun=\"medpolish\",verbose=T)\n",
    "  exprs(allProts) <- log2(exprs(allProts))\n",
    "   \n",
    "    \n",
    "  print(head(exprs(allProts)))\n",
    "    \n",
    "  boxplot(exprs(allProts))\n",
    "    \n",
    "  #stop(\"\")\n",
    "  #pairs(exprs(allProts))\n",
    "  #save\n",
    "  write.exprs(allProts,file=paste(\"/home/biodocker/OUT/\",s,\"/AllQuantProteins.csv\",sep=\"\"))\n",
    "  \n",
    "  ProtDat[[s]] <- allProts <- exprs(allProts)\n",
    "  pca <- princomp(allProts[complete.cases((allProts)),])\n",
    "  plot(pca)\n",
    "  plot(pca$loadings)\n",
    "  text(pca$loadings,colnames(ProtDat))\n",
    "  \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Hide code",
  "hide_code_all_hidden": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "notify_time": "5"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
